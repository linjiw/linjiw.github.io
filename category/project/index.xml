<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Project | Linji Wang</title><link>https://linjiw.github.io/category/project/</link><atom:link href="https://linjiw.github.io/category/project/index.xml" rel="self" type="application/rss+xml"/><description>Project</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 05 Feb 2019 00:00:00 +0000</lastBuildDate><image><url>https://linjiw.github.io/media/icon_hub02f11719440bf94480d1ab9e24c040b_151872_512x512_fill_lanczos_center_3.png</url><title>Project</title><link>https://linjiw.github.io/category/project/</link></image><item><title>Colorizing the Prokudin-Gorskii Photo Collection</title><link>https://linjiw.github.io/post/colorizing-the-prokudin-gorskii-photo-collectiontickets/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://linjiw.github.io/post/colorizing-the-prokudin-gorskii-photo-collectiontickets/</guid><description>&lt;!-- raw HTML omitted -->
&lt;details class="toc-inpage d-print-none " open>
&lt;summary class="font-weight-bold">Table of Contents&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#overview">Overview&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#matching-metric">Matching Metric&lt;/a>&lt;/li>
&lt;li>&lt;a href="#efficiency">Efficiency&lt;/a>&lt;/li>
&lt;li>&lt;a href="#bells--whistles">Bells &amp;amp; Whistles&lt;/a>&lt;/li>
&lt;li>&lt;a href="#extra-credit">Extra Credit&lt;/a>&lt;/li>
&lt;li>&lt;a href="#final-results">Final Results&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;h1 id="colorizing-the-prokudin-gorskii-photo-collection">Colorizing the Prokudin-Gorskii Photo Collection&lt;/h1>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>This project requires you to use image processing techniques to create a color image from the digitized Prokudin-Gorskii glass plate photographs, with the goal of producing an image with as few visual artifacts as possible. A digital picture of a glass plate, with its three channels arranged from top to bottom as BGR (see Figure 1), serves as the process&amp;rsquo;s input. Our task is to isolate the three channels and properly align them.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cathedral.jpg" alt="cathedral" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 1. Digitized Glass Plate: Cathedral&lt;/p>
&lt;h3 id="matching-metric">Matching Metric&lt;/h3>
&lt;p>Image matching means to manipulate image to ensure the best results for your similarity metrics. Thus, it is important to choose an efficient and accurate image matching metrics. For this assignment, I explored Sum of Squared Differences (SSD) and Normalized Cross Correlation (NCC) functions. The objective is to maximize or minimize one of these functions by searching and manipulating the images.&lt;/p>
&lt;p>&lt;strong>Sum of Squared Differences&lt;/strong> SSD is calculated based on the following equation:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/ssd.png" alt="ssd" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>where &lt;em>I&lt;/em> and &lt;em>H&lt;/em> are function of two images and x,y are pixels positions.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/11_13PM_February_14_2023/cathedral.jpg_square_limit25_CROPFalse_CannyTrue_PyramidFalse_CMEASFalse_methodSSD_shift[[0,%20-25],[7,%20-25]]_time_cost%202.1364333629608154.jpg" alt="cathedral_ssd" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 2. Cathedral: SSD, Shift:[0, -25],[7, -25] time cost: 2.13s&lt;/p>
&lt;p>&lt;strong>Normalized Cross Correlation&lt;/strong> NNC is calculated based on the following equation:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/ncc.png" alt="ncc" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>where \mu I and\mu H are the luminance of each image.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/11_13PM_February_14_2023/cathedral.jpg_square_limit25_CROPFalse_CannyTrue_PyramidFalse_CMEASFalse_methodNCC_shift%5B%5B0%2C%20-25%5D%2C%5B7%2C%20-25%5D%5D_time_cost%209.591359376907349.jpg" alt="cathedral_ncc" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 3. Cathedral: NCC, Shift:[0, -25],[7, -25] time cost: 9.59s&lt;/p>
&lt;h3 id="efficiency">Efficiency&lt;/h3>
&lt;p>In the above implementation, we construct two for loops to search for the best position in a small window. Comparing and searching image in a small image might work; however, it will take minutes to process a 3500 by 3500 image. We need additional algorithms to pursue efficiency:&lt;/p>
&lt;p>&lt;strong>Image Pyramid Search&lt;/strong> This is an efficient search algorithm that generally yields good results in less than 60 seconds for high resolution imagesâ€”up to 9000 pixels. As mentioned above, this algorithm scales down the image by &lt;em>scale_factor&lt;/em> times, being &lt;em>scale_factor&lt;/em> an automated parameter based on the input image resolution. Given an initial window search, the algorithm searches from coarse to fine images, within the reduced window size, which decreases by a hyperparameter of &lt;em>window_size&lt;/em> as the images become larger.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/pyramid.jpg" alt="pyramid" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Let&amp;rsquo;s first see how long will it take if we process a 3000 by 3000 image without pyramid search.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/train.tifsquare_limit50_CROPFalse_CannyFalse_PyramidFalse_searchFalse_methodSSD.jpg" alt="train_pyramid" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 4. Train: SSD w/o Pyramid, time cost: 1857s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/train.tifsquare_limit50_CROPFalse_CannyFalse_PyramidFalse_searchFalse_methodNCC.jpg" alt="train_pyramid_ncc" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 5. Train: NCC w/o Pyramid, time cost: 4486s&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Let&amp;rsquo;s apply the pyramid search to see how it affect the searching.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/train.tifsquare_limit1_CROPFalse_CannyFalse_PyramidTrue_searchFalse_methodSSD.jpg" alt="train" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 6. Train: SSD with Pyramid, time cost: 19.03s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/train.tifsquare_limit1_CROPFalse_CannyFalse_PyramidTrue_searchFalse_methodNCC.jpg" alt="train" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 7. Train: NCC with Pyramid, time cost: 39.41s&lt;/p>
&lt;p>Wow, I believe that is a huge performance increase. But the image quality seems not good enough. We will introduce a few of methods in the following part.&lt;/p>
&lt;p>The following are the results with the Pyramid method.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h3 id="bells--whistles">Bells &amp;amp; Whistles&lt;/h3>
&lt;p>&lt;strong>Auto Border Crop&lt;/strong>&lt;/p>
&lt;p>The borders seem very annoying: 1. They don&amp;rsquo;t contribute to the final results. 2. They can even mess our matching metric.&lt;/p>
&lt;p>Let&amp;rsquo;s try to build a simple border corp algo to handle it.&lt;/p>
&lt;ul>
&lt;li>
&lt;ol>
&lt;li>Define what is a border. Border usually is a long vertical or horizontal distinction between two parts. We can use this attribute as a start.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;ol start="2">
&lt;li>Detect the distinctions. Since the distinction is a huge difference between two parts, they can surely be detected by the edge detector. We utilize Canny Edge Detector for this problem, and define the longgest horizontal/vertical edge as the border. If the edge counts more than 90% of the height or width, we define it as the border and use the ratio as the crop ratio.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>Let&amp;rsquo;s see some images after we implementing the auto border crop.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/train.tifsquare_limit1_CROPTrue_CannyFalse_PyramidTrue_searchFalse_methodSSD.jpg" alt="train" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 18. Train: SSD with Pyramid, CROP, time cost: 34.05s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/train.tifsquare_limit1_CROPTrue_CannyFalse_PyramidTrue_searchFalse_methodNCC.jpg" alt="train" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 19. Train: NCC with Pyramid, CROP, time cost: 44.67s&lt;/p>
&lt;p>The border got cropped before and after the image matching to ensure a clean image. However, the channels are still not matched very well.&lt;/p>
&lt;p>&lt;strong>Edge Detector&lt;/strong>&lt;/p>
&lt;p>Maybe pixel intensity is not enough for the matching, other features like edges are also important. This time, we use Canny edges for SSD and NCC for image matching.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/train.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchFalse_methodSSD.jpg" alt="train" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 20. Train: SSD with Pyramid, CROP, Canny, time cost: 18.30s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/train.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchFalse_methodNCC.jpg" alt="train" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 21. Train: NCC with Pyramid, CROP, Canny, time cost: 43.35s&lt;/p>
&lt;h3 id="extra-credit">Extra Credit&lt;/h3>
&lt;p>&lt;strong>Evolution Method: Covariance matrix adaptation evolution strategy (CMA-ES)&lt;/strong> The Exhaustive Search can only work for small images and small window size, due to its time complexity of O(windowsize)^2. I propose a novel optimization approach to search the best alignment with CMA-ES. This black-box optimization algorithm consists on treating the search space as a multivariate Gaussian, with varying mean and covariance matrix. The algorithm starts with a population P, initial x and y movement, and initial covariance. We select elites by keeping a fraction of the population of 10% at each iteration, and noise epsilon is added to covariance at each step. We let the algorithm run for several iterations based on image size and set an initial Gaussian variance corroesponding to the windowsize. We optimize using the following equations:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/evolutionsearch.png" alt="evolutionsearch" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Let&amp;rsquo;s see this cool GIF to have an idea how the evolution method find the optimal values.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/CMEASanime.gif" alt="CMEASanime" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 22. CMEAS parameter update process&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/IMGMOVEanime.gif" alt="IMGMOVEanime" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 23. CMEAS image update process&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/train.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchTrue_methodSSD.jpg" alt="train" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 24. Train: SSD with Pyramid, Canny, CMEAS, CROP, time cost: 12.89s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/train.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchTrue_methodNCC.jpg" alt="train" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 25. Train: NCC with Pyramid, Canny, CMEAS, CROP, time cost: 25.77s&lt;/p>
&lt;p>The CMEAS Evolution method brings 35% performance increase with more clear results.&lt;/p>
&lt;h3 id="final-results">Final Results&lt;/h3>
&lt;p>![cathedral](./data/cathedral.jpg_square_limit10_CROPTrue_CannyTrue_PyramidTrue_CMEASTrue_methodNCC_shift[[5, 2],[12, 1]]_time_cost 1.9317255020141602.jpg)&lt;/p>
&lt;p>Figure 26. cathedral: shift[[5, 2],[12, 1]]_time_cost 1.93s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/emir.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchTrue_methodNCC.jpg" alt="emir" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 27. emir: shift[[38, 16],[91, 18]]_time_cost 31.65s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/harvesters.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchTrue_methodNCC.jpg" alt="harvesters" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 28. harvesters: shift[[61, 13],[123, 13]]_time_cost 40.10s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/icon.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchTrue_methodNCC.jpg" alt="icon" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 29. icon: shift[[30, 16],[76, 22]]_time_cost 33.83s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/lady.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchTrue_methodNCC.jpg" alt="lady" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 30. lady: shift[[52, 7],[120, 4]]_time_cost 31.90s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/self_portrait.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchTrue_methodNCC.jpg" alt="self_portrait" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 31. self_portrait: shift[[74, 25],[167, 19]]_time_cost 32.29s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/three_generations.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchTrue_methodNCC.jpg" alt="three_generations" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 32. three_generations: shift[[44, 6],[106, 6]]_time_cost 38.12s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/turkmen.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchTrue_methodNCC.jpg" alt="turkmen" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 33. turkmen: shift[[56, 20],[105, 21]]_time_cost 34.34s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/village.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchTrue_methodNCC.jpg" alt="village" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 34. village: shift[[63, 3],[136, 20]]_time_cost 38.50s&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/train.tifsquare_limit1_CROPTrue_CannyTrue_PyramidTrue_searchTrue_methodNCC.jpg" alt="train" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Figure 35. train: shift[[41, 0],[78, 23]]_time_cost 46.49s&lt;/p></description></item><item><title>Explore Test 2 Image Generating</title><link>https://linjiw.github.io/post/project/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://linjiw.github.io/post/project/</guid><description>&lt;!-- raw HTML omitted -->
&lt;details class="toc-inpage d-print-none " open>
&lt;summary class="font-weight-bold">Table of Contents&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#start-with-gigagan">Start with GigaGAN&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#challenges-in-gigagan">Challenges in GigaGAN&lt;/a>&lt;/li>
&lt;li>&lt;a href="#results-in-gigagan">Results in GigaGAN&lt;/a>&lt;/li>
&lt;li>&lt;a href="#conclusions-in-gigagan">Conclusions in GigaGAN:&lt;/a>&lt;/li>
&lt;li>&lt;a href="#continue-explore-text-2-image">Continue Explore Text 2 Image&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#switch-to-imagen">Switch to Imagen&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#implementation">Implementation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#experimentation">Experimentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-work-">Future Work ðŸ’¡&lt;/a>&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;h1 id="project-report-test-2-image-generating-task-">Project Report: Test 2 Image Generating Task ðŸŽ¨ðŸ–¼ï¸&lt;/h1>
&lt;p>Author: Zhiyi Shi (&lt;a href="zhiyis@andrew.cmu.edu">zhiyis&lt;/a>), Linji Wang (&lt;a href="linjiw@andrew.cmu.edu">linjiw&lt;/a>)
Github: &lt;a href="https://linjiw.github.io/Explore_Text2Image/" target="_blank" rel="noopener">Github Page&lt;/a>, &lt;a href="https://github.com/linjiw/MinImagen" target="_blank" rel="noopener">Github repository&lt;/a>&lt;/p>
&lt;h2 id="start-with-gigagan">Start with GigaGAN&lt;/h2>
&lt;!-- raw HTML omitted -->
&lt;p>Once upon a time, in our class, we were introduced to &lt;strong>GigaGAN&lt;/strong>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> - a remarkable idea that instantly caught our attention. As we delved deeper into the topic, we were astounded by the intricate handling of the filter bank that was mentioned. Our curiosity was piqued, and we found ourselves deeply interested in the concept. Moreover, we were already drawn to the idea of text-to-image conversion and its potential applications. We felt compelled to explore the intricacies of Giga GAN further and decided to undertake a project to reproduce the code and learn more about its text-to-image processing techniques.&lt;/p>
&lt;p>
&lt;figure id="figure-gigagan">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="data/GigaGAN%20model.png" alt="GigaGAN" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
GigaGAN
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h3 id="challenges-in-gigagan">Challenges in GigaGAN&lt;/h3>
&lt;p>However, after reading the entire paper, we realized the serious shortage of computing resources. (The smallest version of Giga GAN in the paper is trained on 64 A-100 GPU.) So, we plan to only create a &amp;ldquo;toy&amp;rdquo; version of Giga GAN to showcase its implementation ideas. After referring to the GitHub code to implement the forward part of the model, we decided to select some parts from the full model in the original paper for experimentation. In the end, we chose clip loss, matching aware loss, multi-scale loss, and filter bank.&lt;/p>
&lt;p>
&lt;figure id="figure-gigagan-model-size">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/gigagan%20model%20size.png" alt="GigaGAN model Size" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
GigaGAN Model Size
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>We have written the code for these parts based on the paper. However, even if we use a very small batch size (which is 2), the model could only be trained on our GPU when incorporating the matching aware loss and clip loss. When multi-scale loss or filter bank is added, the GPU&amp;rsquo;s memory is insufficient.&lt;/p>
&lt;p>The dataset we use is a Pokemon dataset with text and images paired. This text will provide a simple description of the image, with key information including color, species, and action.&lt;/p>
&lt;p>
&lt;figure id="figure-pokemon-dataset">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/pokemon_dataset_small.png" alt="Pokemon Dataset" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Pokemon Dataset
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h3 id="results-in-gigagan">Results in GigaGAN&lt;/h3>
&lt;p>In the beginning, we only added the most basic GAN loss without any condition to test the ability to generate clear images: that is, the model only pays attention to the authenticity of the generated images.&lt;/p>
&lt;p>
&lt;figure id="figure-pokemon-unconditional">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/pokemon%20unconditional.png" alt="Pokemon Unconditional" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Pokemon Unconditional
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>It can be seen that the generated image still has a rough outline, although the details are not satisfactory. Also, it can be seen that the colors, contours, and patterns all have certain rules, and the generated images have diversity.&lt;/p>
&lt;p>Then we tried to add a naÃ¯ve text condition to observe the results:&lt;/p>
&lt;p>
&lt;figure id="figure-drawing-of-a-brown-and-black-pokemon">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/drawing%20of%20a%20brown%20and%20black%20Pokemon.png" alt="drawing of a brown and black Pokemon" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
drawing of a brown and black Pokemon
&lt;/figcaption>&lt;/figure>
&lt;strong>drawing of a brown and black Pokemon&lt;/strong>&lt;/p>
&lt;p>It can be seen that the image quality has significantly decreased compared to not incorporating text conditions. It can be seen that the generated image roughly matches the content described in the text, but its shape is relatively strange. It means that the addition of text conditions significantly makes training more difficult to converge.&lt;/p>
&lt;p>Then we added matching loss for training and observed the results. It can be seen that the colors described in the generated image and text can match, but the shape is relatively irregular, indicating that the training did not converge well.&lt;/p>
&lt;p>
&lt;figure id="figure-drawing-of-a-brown-and-black-pokemon">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/drawing%20of%20a%20brown%20and%20black%20Pokemon%202.png" alt="drawing of a brown and black Pokemon 2" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
drawing of a brown and black Pokemon
&lt;/figcaption>&lt;/figure>
&lt;strong>drawing of a brown and black Pokemon with matching loss&lt;/strong>&lt;/p>
&lt;p>Then we added clip loss for training and observed the results. The generated images have no meaning, this is a completely non-convergent model. This indicates that our designed clip loss makes training extremely difficult to converge.&lt;/p>
&lt;p>
&lt;figure id="figure-drawing-of-a-brown-and-black-pokemon-with-clip">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="data/drawing%20of%20a%20brown%20and%20black%20Pokemon%20clip.png" alt="drawing of a brown and black Pokemon 3" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
drawing of a brown and black Pokemon with clip
&lt;/figcaption>&lt;/figure>
&lt;strong>drawing of a brown and black Pokemon with clip loss&lt;/strong>&lt;/p>
&lt;p>Besides the visual differences, the quantitative results also show that the clip loss did not work in our case, and even produce worse results.&lt;/p>
&lt;p>
&lt;figure id="figure-lpips">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/LPIPS.png" alt="LPIPS Results" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
LPIPS
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>This experiment is done using &lt;a href="https://github.com/richzhang/PerceptualSimilarity" target="_blank" rel="noopener">&lt;strong>Learned Perceptual Image Patch Similarity (LPIPS)&lt;/strong>&lt;/a> for evaluate the simiarity in feature space.&lt;/p>
&lt;h3 id="conclusions-in-gigagan">Conclusions in GigaGAN:&lt;/h3>
&lt;ol>
&lt;li>The effect of the image we generated is not good, which may be related to the selection of a hyperparameter and the size of the data. Firstly, choosing a smaller batch size can result in poor convergence of the entire training. Secondly, there are too many hyperparameters in this model, and we can only use a few tuning parameters due to our computing resources.&lt;/li>
&lt;li>GANs, in general, require a substantial amount of computing resources for training and might be unstable when training on small datasets or with insufficient computational resources.&lt;/li>
&lt;/ol>
&lt;h3 id="continue-explore-text-2-image">Continue Explore Text 2 Image&lt;/h3>
&lt;p>Although the implementation effect of Giga GAN is not satisfactory, we are still quite interested in handling clip loss. Thus, we searched for relevant papers and open-source resources and decided to convert text to images again on the diffusion model, and compared its effectiveness with GAN.&lt;/p>
&lt;h2 id="switch-to-imagen">Switch to Imagen&lt;/h2>
&lt;p>In this project, we first attempted to reproduce and explore the GigaGAN model, uncovering its remarkable text-to-image processing techniques. Subsequently, we shifted our focus to the &lt;strong>MinImagen&lt;/strong>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> architecture, an efficient text-to-image generation model capable of producing high-quality images. Our goal is to experiment with these two models, comparing their strengths and weaknesses, and ultimately share our findings and experiences in the world of image generation.&lt;/p>
&lt;p>
&lt;figure id="figure-imagen-model">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/Imagen_model_structure.png" alt="Imagen Architechture" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Imagen Model
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h3 id="implementation">Implementation&lt;/h3>
&lt;p>We began by implementing the &lt;strong>MinImagen&lt;/strong> model using the resources provided by &lt;strong>AssemblyAI&lt;/strong> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. The key component of this implementation is the &lt;strong>Diffusion&lt;/strong> part, which has the following characteristics compared with the &lt;strong>GigaGAN&lt;/strong> in our training environment.:&lt;/p>
&lt;ul>
&lt;li>Training is fast âš¡&lt;/li>
&lt;li>Sample image generation is slow ðŸ¢&lt;/li>
&lt;li>Convergence is fast ðŸƒ&lt;/li>
&lt;li>Generated results are more meaningful ðŸŽ¯&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h3 id="experimentation">Experimentation&lt;/h3>
&lt;p>During the experimentation phase, we encountered some challenges:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Model Size&lt;/strong>: The original model with super resolution was too large to fit into a 12GB GPU for training, even with a batch size of 1.&lt;/li>
&lt;li>&lt;strong>Logging&lt;/strong>: We used &lt;strong>Weights &amp;amp; Biases (wandb)&lt;/strong> for logging our training process.&lt;/li>
&lt;/ol>
&lt;p>
&lt;figure id="figure-validation-loss">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Imagen Training" srcset="
/post/project/data/Imagen_training_hua41c1c38400bea9d0b4e8558892b5dc1_80236_6fc326ee699feb087a5a94cc4d401ee5.webp 400w,
/post/project/data/Imagen_training_hua41c1c38400bea9d0b4e8558892b5dc1_80236_30cc487228e658bd5aade5286e9048bc.webp 760w,
/post/project/data/Imagen_training_hua41c1c38400bea9d0b4e8558892b5dc1_80236_1200x1200_fit_q100_h2_lanczos_3.webp 1200w"
src="https://linjiw.github.io/post/project/data/Imagen_training_hua41c1c38400bea9d0b4e8558892b5dc1_80236_6fc326ee699feb087a5a94cc4d401ee5.webp"
width="760"
height="469"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Validation Loss
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;ol start="3">
&lt;li>&lt;strong>Remove Super Resolution&lt;/strong>: Due to the model&amp;rsquo;s large size, we had to remove the super resolution layer for training.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Training parameters
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;text_embed_dim&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">null&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;channels&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">3&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;timesteps&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">1000&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;cond_drop_prob&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">0.15&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;loss_type&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;l2&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;lowres_sample_noise_level&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">0.2&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;auto_normalize_img&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;dynamic_thresholding_percentile&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">0.9&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;only_train_unet_number&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">null&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;image_sizes&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">64&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;text_encoder_name&amp;#34;&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;t5_small&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">// Model Size
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;dim&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">128&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;dim_mults&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">1&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">2&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">4&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;channels&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">3&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;channels_out&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">null&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;cond_dim&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">null&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;text_embed_dim&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">512&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;num_resnet_blocks&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;layer_attns&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">false&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;layer_cross_attns&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">false&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;attn_heads&amp;#34;&lt;/span>: &lt;span style="color:#ae81ff">8&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;lowres_cond&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">false&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;memory_efficient&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">false&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;#34;attend_at_middle&amp;#34;&lt;/span>: &lt;span style="color:#66d9ef">false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Despite these challenges, our model was able to converge quickly, taking only 300 epochs with a batch size of 2 and a time step of 1000 to generate meaningful images. ðŸŒŸ&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Training Step&lt;/th>
&lt;th style="text-align:center">Image&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">a blue and red pokemon 500&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/imagen_500.png" alt="Step 500 Image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">a blue and red pokemon 600&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/imagen_600.png" alt="Step 600 Image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">a blue and red pokemon 700&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/imagen_700.png" alt="Step 700 Image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">a blue and red pokemon 800&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/imagen_800.png" alt="Step 800 Image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We think the quality of Imagen generated image is better than our previous results from GigaGAN, even in a very early training stage. One could think these images look more complete than the previous results, thus some extent more like a Pokemon.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Text Prompt (text scale)&lt;/th>
&lt;th style="text-align:center">Image&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">a drawing of a green pokemon with red eyes (0.1)&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/text_scale_0.1.png" alt="text scale = 0.1" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">a drawing of a green pokemon with red eyes (3.0)&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/text_scale_3.0.png" alt="text scale = 3.0" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">a drawing of a green pokemon with red eyes (5.0)&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/text_scale_5.0.png" alt="text scale = 5.0" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We could notice that with add more strength to the text scale, it starts to align with the text in some extent. Like when it is 0.1, it is just green. When add to 3.0, it shows a yellow color, which might be some blending information from &amp;ldquo;blue&amp;rdquo; and &amp;ldquo;red&amp;rdquo;. When add to 5.0, it shows a blend with yellow and green, thus we assume it catchs more information from the content step by step.&lt;/p>
&lt;p>When text scale = 5.0, we could notice that the image quality is not good and has a lot of noise on the white background. This is due to the text scale is too high that we force the network to focus more on the text information, but not the quality side.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h3 id="future-work-">Future Work ðŸ’¡&lt;/h3>
&lt;p>We plan to add a &lt;strong>super resolution layer&lt;/strong> in the future to further improve our image generation capabilities.&lt;/p>
&lt;h3 id="conclusion">Conclusion&lt;/h3>
&lt;p>Our project on implementing and experimenting with the MinImagen architecture for text-to-image generation has been successful. We were able to generate meaningful images from textual descriptions, overcoming challenges related to model size and training resources. We hope that our experience and findings can help others working on similar projects. ðŸ˜ƒ&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>GigaGAN (&lt;a href="https://mingukkang.github.io/GigaGAN/" target="_blank" rel="noopener">https://mingukkang.github.io/GigaGAN/&lt;/a>)&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>MinImagen: Build Your Own Imagen Text-to-Image Model (&lt;a href="https://www.assemblyai.com/blog/minimagen-build-your-own-imagen-text-to-image-model/" target="_blank" rel="noopener">https://www.assemblyai.com/blog/minimagen-build-your-own-imagen-text-to-image-model/&lt;/a>)&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>AssemblyAI-Examples/MinImagen GitHub Repository (&lt;a href="https://github.com/AssemblyAI-Examples/MinImagen?ref=assemblyai.com" target="_blank" rel="noopener">https://github.com/AssemblyAI-Examples/MinImagen?ref=assemblyai.com&lt;/a>)&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>GAN Photo Editing</title><link>https://linjiw.github.io/post/gan-editing/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://linjiw.github.io/post/gan-editing/</guid><description>&lt;!-- raw HTML omitted -->
&lt;details class="toc-inpage d-print-none " open>
&lt;summary class="font-weight-bold">Table of Contents&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#setup">Setup&lt;/a>&lt;/li>
&lt;li>&lt;a href="#part-1-inverting-the-generator-30-pts">Part 1: Inverting the Generator [30 pts]&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#implementation-details">Implementation Details&lt;/a>&lt;/li>
&lt;li>&lt;a href="#deliverables">Deliverables&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#part-2-interpolate-your-cats-10-pts">Part 2: Interpolate your Cats [10 pts]&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#implementation">Implementation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#deliverables-1">Deliverables&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#part-3-scribble-to-image-40-points">Part 3: Scribble to Image [40 Points]&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#implementation-details-1">Implementation Details&lt;/a>&lt;/li>
&lt;li>&lt;a href="#deliverables-2">Deliverables&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conlusions">Conlusions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In this assignment, we implement a few different techniques that require manipulating images on the manifold of natural images.&lt;/p>
&lt;ul>
&lt;li>First, we invert a pre-trained generator to find a latent variable that closely reconstructs a given real image.&lt;/li>
&lt;li>In the second part of the assignment, we take a hand-drawn sketch and generate an image that fits the sketch accordingly.&lt;/li>
&lt;/ul>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>To set up the environment for this project, create a new virtual environment and install the required dependencies:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>conda create &lt;span style="color:#f92672">-&lt;/span>n &lt;span style="color:#ae81ff">16726&lt;/span>_hw5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>conda activate &lt;span style="color:#ae81ff">16726&lt;/span>_hw5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip3 install torch&lt;span style="color:#f92672">==&lt;/span>&lt;span style="color:#ae81ff">1.12.1&lt;/span>&lt;span style="color:#f92672">+&lt;/span>cu113 torchvision&lt;span style="color:#f92672">==&lt;/span>&lt;span style="color:#ae81ff">0.13.1&lt;/span>&lt;span style="color:#f92672">+&lt;/span>cu113 torchaudio&lt;span style="color:#f92672">==&lt;/span>&lt;span style="color:#ae81ff">0.12.1&lt;/span> &lt;span style="color:#f92672">--&lt;/span>extra&lt;span style="color:#f92672">-&lt;/span>index&lt;span style="color:#f92672">-&lt;/span>url https:&lt;span style="color:#f92672">//&lt;/span>download&lt;span style="color:#f92672">.&lt;/span>pytorch&lt;span style="color:#f92672">.&lt;/span>org&lt;span style="color:#f92672">/&lt;/span>whl&lt;span style="color:#f92672">/&lt;/span>cu113
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip3 install click requests tqdm pyspng ninja matplotlib imageio imageio&lt;span style="color:#f92672">-&lt;/span>ffmpeg&lt;span style="color:#f92672">==&lt;/span>&lt;span style="color:#ae81ff">0.4.3&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pip install wandb &lt;span style="color:#75715e"># weight and bias is used in this blog for logging experiments.&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="part-1-inverting-the-generator-30-pts">Part 1: Inverting the Generator [30 pts]&lt;/h2>
&lt;p>In the first part of the assignment, we solve an optimization problem to reconstruct the image from a particular latent code. We use different combinations of loss functions, generative models, and latent spaces to find the best result.&lt;/p>
&lt;h3 id="implementation-details">Implementation Details&lt;/h3>
&lt;ol>
&lt;li>Implement the forward function in the &lt;code>Criterion&lt;/code> class.&lt;/li>
&lt;li>Implement &lt;code>sample_noise&lt;/code> for StyleGAN2, including w and w+.&lt;/li>
&lt;li>Implement the optimization step using LBFGS or other optimizers.&lt;/li>
&lt;li>Implement the whole functionality in &lt;code>project()&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h3 id="deliverables">Deliverables&lt;/h3>
&lt;p>Show example outputs of image reconstruction efforts and provide comments on why the various outputs look how they do.&lt;/p>
&lt;ul>
&lt;li>Various combinations of the losses including Lp loss, Preceptual loss and/or regularization loss that penalizes L2 norm of delta.&lt;/li>
&lt;li>different generative models including vanilla GAN, StyleGAN&lt;/li>
&lt;li>different latent space (latent code in z space, w space, and w+ space)&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">L1 Loss&lt;/th>
&lt;th style="text-align:center">Perceptual Loss&lt;/th>
&lt;th style="text-align:center">Regularization Loss&lt;/th>
&lt;th style="text-align:center">Model&lt;/th>
&lt;th style="text-align:center">Latent Space&lt;/th>
&lt;th style="text-align:center">Results&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">Vanilla GAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/vanilla_z_1_100_1e_06.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">Vanilla GAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/vanilla_z_1_100_0.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">Vanilla GAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/vanilla_z_1_0_1e-06.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">Vanilla GAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/vanilla_z_1_0_0.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">Vanilla GAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_vanilla_z_0_100_1e-06.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">Vanilla GAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_vanilla_z_0_100_0.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_z_1_100_1e-06_4004_bebeb425d957dbef4f9c.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_z_1_100_0_4004_415e536104247044383b.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_z_1_0_1e-06_4004_c94a7b60cb156f074e48.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_z_1_0_0_4004_dd13a0dc4a4ab66f66e3.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_z_0_100_1e-06_4004_bc0458e560bd161434dc.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">z&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_z_0_100_0_4004_72dbd55809d60c6aadf5.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_w_1_100_1e-06_4004_b631e25d5521c00718d4.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_w_1_100_0_4004_829d13f5a063b2c278d0.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_w_1_0_1e-06_4004_068ef97c4f13a85ca22b.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_w_1_0_0_4004_a0048096f02d312a464c.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_w_0_100_1e-06_4004_0011bfc0a88246482962.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_w_0_100_0_4004_b9fc403d056cb3d20782.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w+&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_w&amp;#43;_1_100_1e-06_4004_d5cb1908a90fef931d42.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w+&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_w&amp;#43;_1_100_0_4004_520c171b3be96c12ec9a.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w+&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_w&amp;#43;_1_0_1e-06_4004_96571a7039c51c03e1a9.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w+&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/0_stylegan_w&amp;#43;_1_0_0_4004_98a74167d8cdde94f7b3.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w+&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/0_stylegan_w&amp;#43;_0_100_1e-06_4004_0011bfc0a88246482962.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">ON&lt;/td>
&lt;td style="text-align:center">OFF&lt;/td>
&lt;td style="text-align:center">StyleGAN&lt;/td>
&lt;td style="text-align:center">w+&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/project/media_images_output_project_0_stylegan_w&amp;#43;_0_100_0_4004_b9fc403d056cb3d20782.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Our experiments compared &lt;strong>GAN architectures&lt;/strong> and &lt;strong>loss functions&lt;/strong> to assess their impact on generated images. We found that &lt;strong>StyleGAN&lt;/strong> with &lt;strong>L1 loss&lt;/strong>, &lt;strong>Perceptual loss&lt;/strong>, and &lt;strong>Regularization loss&lt;/strong> consistently delivered superior results, generating high-quality images closely resembling the target distribution.&lt;/p>
&lt;p>We observed challenges in training without &lt;strong>Perceptual loss&lt;/strong>, resulting in less stable training processes. In contrast, &lt;strong>Vanilla GAN&lt;/strong> generated plausible images but lacked the fine-grained detail present in &lt;strong>StyleGAN&lt;/strong> outputs.&lt;/p>
&lt;p>In conclusion, &lt;strong>StyleGAN&lt;/strong> combined with &lt;strong>L1&lt;/strong>, &lt;strong>Perceptual&lt;/strong>, and &lt;strong>Regularization losses&lt;/strong> outperformed other configurations, demonstrating its effectiveness in generating high-quality, detailed images.&lt;/p>
&lt;h2 id="part-2-interpolate-your-cats-10-pts">Part 2: Interpolate your Cats [10 pts]&lt;/h2>
&lt;p>In this part, we perform interpolation between latent vectors found in Part 1 using different generative models and latent spaces.&lt;/p>
&lt;h3 id="implementation">Implementation&lt;/h3>
&lt;ol>
&lt;li>Implement the interpolation step in &lt;code>interpolate()&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h3 id="deliverables-1">Deliverables&lt;/h3>
&lt;p>Show a few interpolations between grumpy cats and comment on the quality of the images and the interpolation process.&lt;/p>
&lt;p>We first generate in the interpolation in 64 by 64.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/interpolate/01.gif" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/interpolate/2.gif" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>But we found 64 by 64 resolution is not enough for website view experience. So we edit part of the code to enable higher resolution (512 by 512).
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/interpolate/03.gif" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/interpolate/5.gif" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>And I tried to test the interpolation on some cute cats images.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/interpolate/11.gif" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/interpolate/15.gif" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="part-3-scribble-to-image-40-points">Part 3: Scribble to Image [40 Points]&lt;/h2>
&lt;p>In this part, we generate an image subject to constraints, like color scribble constraints, using a penalized nonconvex optimization problem.&lt;/p>
&lt;h3 id="implementation-details-1">Implementation Details&lt;/h3>
&lt;ol>
&lt;li>Implement the code for synthesizing images from drawings to realistic ones using the optimization procedure in &lt;code>draw()&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h3 id="deliverables-2">Deliverables&lt;/h3>
&lt;p>Draw some cats and experiment with sparser and denser sketches and the use of color. Show example outputs along with commentary on what seems to have happened and why.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Target&lt;/th>
&lt;th style="text-align:center">Results&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_draw_0_data_0_685759aa7ccdce514a24.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_project_0_stylegan_w&amp;#43;_1_10_1e-06_12013_08c54f90bce097455075.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_draw_1_data_12062_00e0913765996185883b.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_project_1_stylegan_w&amp;#43;_1_10_1e-06_24075_c541cd2f1e2be1d82034.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/2_data_24100_d786d60d1e48ec2aebf2.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_project_2_stylegan_w&amp;#43;_1_10_1e-06_36113_f03f9cffdc953145278d.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_draw_3_data_36130_6976808218d3e438a312.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_project_3_stylegan_w&amp;#43;_1_10_1e-06_48143_1be8665e33a1bf3150e8.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_draw_4_data_48152_554a7c1d62c815e83243.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_project_4_stylegan_w&amp;#43;_1_10_1e-06_60165_e3516c0d8ce51c0011fc.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_draw_5_data_60178_bd3aaa9115213ce0d768.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_project_5_stylegan_w&amp;#43;_1_10_1e-06_72191_f2ed53d8e93e33c07ba8.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_draw_6_data_72196_e294ed27fb37e4e191cf.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_project_6_stylegan_w&amp;#43;_1_10_1e-06_84209_fd14e62000b27732a369.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_draw_7_data_84230_062814c6a7cee9aa5e24.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_project_7_stylegan_w&amp;#43;_1_10_1e-06_96243_2946dc0f8311277108e4.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_draw_8_data_96248_0e319a9a812700472a55.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_project_8_stylegan_w&amp;#43;_1_10_1e-06_108261_fa18dcf2db0b95732dfe.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>I also add some DIY mask for fun.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Target&lt;/th>
&lt;th style="text-align:center">Results&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_draw_diy_data_0_7b736518cfeab1bf8f0f.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/draw/media_images_output_project_diy_stylegan_w&amp;#43;_1_10_1e-06_12013_f20ce2182199539dfb88.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Our recent experiments explored the capability of our implementation to generate &lt;strong>intriguing images&lt;/strong> that closely conform to the &lt;strong>target drawing masks&lt;/strong>. The results showed that our approach successfully produced visually appealing images that adhered to the provided masks.&lt;/p>
&lt;p>However, we observed that some generated images appeared dark due to the darkness of the corresponding masks. While the overall results were impressive, it&amp;rsquo;s worth noting that the &lt;strong>mask&amp;rsquo;s darkness&lt;/strong> can impact the final image&amp;rsquo;s brightness and contrast.&lt;/p>
&lt;p>In conclusion, our experiments demonstrated that our implementation could effectively generate interesting images that align with the target drawing masks, though the mask&amp;rsquo;s darkness may &lt;strong>influence the resulting image&amp;rsquo;s appearance&lt;/strong>.&lt;/p>
&lt;h2 id="conlusions">Conlusions&lt;/h2>
&lt;p>Throughout this project, we have investigated various aspects of image generation using &lt;strong>GAN architectures&lt;/strong>. ðŸ–¼ï¸&lt;/p>
&lt;p>&lt;strong>Part 1&lt;/strong> focused on comparing different GAN architectures and loss functions, where &lt;strong>StyleGAN&lt;/strong> with &lt;strong>L1 loss&lt;/strong>, &lt;strong>Perceptual loss&lt;/strong>, and &lt;strong>Regularization loss&lt;/strong> proved to be the most effective in generating high-quality, detailed images. We also observed challenges in training without &lt;strong>Perceptual loss&lt;/strong> ðŸ˜…, and found that &lt;strong>Vanilla GAN&lt;/strong> could not match the fine-grained detail present in &lt;strong>StyleGAN&lt;/strong> outputs.&lt;/p>
&lt;p>In &lt;strong>Part 2&lt;/strong>, we showcased several interpolations between grumpy cat images ðŸ±, initially at a resolution of 64x64, which was later increased to 512x512 for a better web viewing experience. The interpolations demonstrated the smooth transitions between images and highlighted the potential of high-resolution image generation ðŸŒŸ.&lt;/p>
&lt;p>&lt;strong>Part 3&lt;/strong> explored generating intriguing images that closely conform to target drawing masks âœï¸. While our implementation successfully produced visually appealing images, we observed that the mask&amp;rsquo;s darkness could impact the final image&amp;rsquo;s brightness and contrast.&lt;/p>
&lt;p>Overall, this project has demonstrated the power of GANs, particularly &lt;strong>StyleGAN&lt;/strong>, in generating high-quality images and interpolations ðŸ”¥. We have also shown the potential of using target drawing masks for image generation, opening up possibilities for further exploration and improvement in image manipulation techniques ðŸ’¡.&lt;/p></description></item><item><title>Gradient Domain Fusion</title><link>https://linjiw.github.io/post/poisson-blending/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://linjiw.github.io/post/poisson-blending/</guid><description>&lt;!-- raw HTML omitted -->
&lt;details class="toc-inpage d-print-none " open>
&lt;summary class="font-weight-bold">Table of Contents&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#gradient-domain-fusion">Gradient Domain Fusion&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#overview">Overview&lt;/a>&lt;/li>
&lt;li>&lt;a href="#toy-problem">Toy Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#poisson-blending">Poisson Blending&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mixed-poisson-blending">Mixed Poisson blending&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mixed-poisson-blending-1">Mixed Poisson blending&lt;/a>&lt;/li>
&lt;li>&lt;a href="#color2gray">Color2Gray&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h2 id="gradient-domain-fusion">Gradient Domain Fusion&lt;/h2>
&lt;p>&lt;em>by Linji Wang, Feb 07, 2023&lt;/em>&lt;/p>
&lt;h3 id="overview">Overview&lt;/h3>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>Welcome to our website about Gradient Domain Fusion, a powerful technique that allows for seamless merging of multiple images into a single high-quality output. Our project aims to explore this technique and provide a detailed guide on how to implement it effectively.&lt;br>
Whether you are a professional photographer or a hobbyist looking to take your images to the next level, our website is the perfect resource to learn about and master Gradient Domain Fusion. So, let&amp;rsquo;s get started and unlock the full potential of this exciting technique!&lt;/p>
&lt;h3 id="toy-problem">Toy Problem&lt;/h3>
&lt;p>In this toy example, we&amp;rsquo;re trying to reconstruct an image called &amp;ldquo;v&amp;rdquo; using some information we get from another image called &amp;ldquo;s&amp;rdquo;. Specifically, we&amp;rsquo;re going to use the x and y gradients of the image s, as well as the intensity of one of its pixels.&lt;/p>
&lt;p>Now, you might be wondering what &amp;ldquo;x and y gradients&amp;rdquo; mean. Think of it this way: imagine looking at a picture of a mountain. If you wanted to describe how the brightness of the image changes as you move your eyes across it, you might say something like &amp;ldquo;the brightness gets darker as you move up the mountain, and lighter as you move down.&amp;rdquo; That&amp;rsquo;s kind of what we mean by &amp;ldquo;gradients&amp;rdquo; - they describe how the brightness (or &amp;ldquo;intensity&amp;rdquo;) of an image changes in different directions.&lt;/p>
&lt;p>So, to summarize: we have one image called &amp;ldquo;s&amp;rdquo;, and we&amp;rsquo;re going to use its x and y gradients (which describe how the brightness changes in different directions) and the intensity of one pixel to create a new image called &amp;ldquo;v&amp;rdquo;. The process isn&amp;rsquo;t too complicated, but it&amp;rsquo;s easy to make mistakes, so we&amp;rsquo;re starting with a simple example to make sure we get it right.&lt;/p>
&lt;p>&lt;strong>Results: toy problem&lt;/strong> Left side: Original image; Right side: Reconstructed Image&lt;/p>
&lt;p>--&amp;gt;
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/toy_reconstruction.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h3 id="poisson-blending">Poisson Blending&lt;/h3>
&lt;p>The first step in Poisson blending is to identify the target region in the image, which is the area where we want to blend the images together. For example, if we have two images of a person and a background, the target region might be the outline of the person.&lt;/p>
&lt;p>Next, we need to construct blending constraints. The goal of these constraints is to ensure that the blended image looks seamless and natural. We do this by making sure that the brightness or intensity of the target region is consistent with the gradients of the source and target images.&lt;/p>
&lt;p>To create these constraints, we use the gradient of the images. The gradient describes how the brightness or intensity of the image changes in different directions. We create a set of equations that relate the gradient of the target region to the gradients of the source and target images. These equations are based on the observation that the gradient of the target region should be equal to the gradient of the source image in the non-target region, so as to ensure smooth blending.&lt;/p>
&lt;p>Once we have the blending constraints, we need to solve a least squares problem to find the values for each pixel in the target region that satisfy these constraints. The solution involves finding the values that minimize the difference between the gradient of the target region and the gradients of the source and target images, subject to the blending constraints. This can be done using numerical optimization methods.&lt;/p>
&lt;p>Finally, we construct the blended image by copying the pixels from the source image into the target region, adjusting their colors and intensities according to the solution of the linear equations. This creates a smooth transition between the target region and the rest of the image, resulting in a final image that looks natural and seamless.&lt;/p>
&lt;p>In summary, Poisson blending involves identifying the target region, constructing blending constraints based on the gradients of the images, solving a least squares problem to find the values for each pixel in the target region, and then constructing the final image by copying the pixels from the source image and adjusting their colors and intensities. The result is a seamless and natural-looking image.&lt;/p>
&lt;p>&lt;strong>Results: poisson blend&lt;/strong> Left side: Naive Blend; Right side: Poisson Blend&lt;/p>
&lt;p>--&amp;gt;
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_01.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A Bear&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_01_newsource_target_01Blend.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A Bear swimming with a girl in a pool&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_2.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A Whale&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_2_newsource_target_2Blend.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A Whale swimming at the sea of Boston&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_03.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A man&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_03_newsource_target_03Blend.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Mona Lisa with a man&amp;rsquo;s face&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_04.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A cat&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_04_newsource_target_04Blend.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A cat with another cat&amp;rsquo;s face&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_05.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A cat&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_05_newsource_target_05Blend.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A cat with another cat&amp;rsquo;s face&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_06.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A snowman&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_06_newsource_target_06Blend.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A snowman standing at The Mall, CMU&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_07.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A painting&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_07_newsource_target_07Blend.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A painting at College of Fine Arts Lawn, CMU&lt;/p>
&lt;h3 id="mixed-poisson-blending">Mixed Poisson blending&lt;/h3>
&lt;p>To elaborate, Mixed Poisson blending is a variation of Poisson blending that is used to blend images with different color channels or color spaces. The process involves identifying the target region, constructing blending constraints, and solving a least squares problem to find the values for each pixel in the target region.&lt;/p>
&lt;p>The blending constraints are based on the idea that the brightness or intensity of the image should be consistent across the boundary between the target region and the rest of the image. In Mixed Poisson blending, these constraints are constructed by taking the greatest gradient in the image and using it to construct a sparse matrix.&lt;/p>
&lt;p>The sparse matrix is used to solve the least squares problem, which involves finding the values for each pixel in the target region that satisfy the blending constraints. The solution involves using iterative methods to find the optimal solution that minimizes the difference between the gradients of the target region and the greatest gradient of the image, subject to the blending constraints.&lt;/p>
&lt;p>The resulting pixel values are combined to create the final blended image, which maintains the colors and textures of each image while appearing seamless and natural.&lt;/p>
&lt;p>In summary, Mixed Poisson blending is used to blend images with different color channels or color spaces. The blending constraints are constructed using the greatest gradient in the image to create a sparse matrix, which is then used to solve the least squares problem and find the pixel values in the target region. The resulting blended image maintains the colors and textures of each image, while appearing seamless and natural. &lt;strong>Results: Mixed poisson blend&lt;/strong> Left side: Naive Blend; Right side: Mixed Poisson Blend&lt;/p>
&lt;h3 id="mixed-poisson-blending-1">Mixed Poisson blending&lt;/h3>
&lt;p>--&amp;gt;
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_01.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A Bear&lt;/p>
&lt;p>![](./data/source_01_newsource_target_01Mixed Blend.jpg)&lt;/p>
&lt;p>A Bear swimming with a girl in a pool&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_2.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A Whale&lt;/p>
&lt;p>![](./data/source_2_newsource_target_2Mixed Blend.jpg)&lt;/p>
&lt;p>A Whale swimming at the sea of Boston&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_03.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A man&lt;/p>
&lt;p>![](./data/source_03_newsource_target_03Mixed Blend.jpg)&lt;/p>
&lt;p>Mona Lisa with a man&amp;rsquo;s face&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_04.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A cat&lt;/p>
&lt;p>![](./data/source_04_newsource_target_04Mixed Blend.jpg)&lt;/p>
&lt;p>A cat with another cat&amp;rsquo;s face&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_05.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A cat&lt;/p>
&lt;p>![](./data/source_05_newsource_target_05Mixed Blend.jpg)&lt;/p>
&lt;p>A cat with another cat&amp;rsquo;s face&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_06.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A snowman&lt;/p>
&lt;p>![](./data/source_06_newsource_target_06Mixed Blend.jpg)&lt;/p>
&lt;p>A snowman standing at The Mall, CMU&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/source_07.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Source: A painting&lt;/p>
&lt;p>![](./data/source_07_newsource_target_07Mixed Blend.jpg)&lt;/p>
&lt;p>A painting at College of Fine Arts Lawn, CMU&lt;/p>
&lt;h3 id="color2gray">Color2Gray&lt;/h3>
&lt;p>The color2gray problem refers to the challenge of converting a color image to a grayscale image while preserving the essential information. Grayscale images are often used in applications such as printing, where color images may not be necessary or may be too expensive to produce.&lt;/p>
&lt;p>One approach to solving the color2gray problem is to convert the image to the HSV (Hue, Saturation, Value) color space. The HSV color space separates color information into three channels: Hue, Saturation, and Value. The Hue channel represents the color itself, while the Saturation and Value channels represent the intensity of the color.&lt;/p>
&lt;p>In the HSV color space, we can notice the color difference between channels, which can help us to preserve the essential information when converting to grayscale. One technique for converting an HSV image to grayscale is to use Mixed Poisson Blending. This technique involves solving a least squares problem that takes into account the greatest gradient in the image to construct a sparse matrix. The sparse matrix is used to find the values for each pixel in the target region, resulting in a grayscale image that maintains the essential information from the original color image.&lt;/p>
&lt;p>In summary, the color2gray problem refers to the challenge of converting a color image to grayscale while preserving the essential information. In the HSV color space, we can notice the color difference between channels, and Mixed Poisson Blending can be used to solve the color2gray problem while maintaining the essential information.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/colorBlindTest35.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A colorBlindTest image&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/rgb2gray_results.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>A Gray colorBlindTest image and its HSV distributions&lt;/p></description></item><item><title>Neural Style Transfer</title><link>https://linjiw.github.io/post/style-optimization/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://linjiw.github.io/post/style-optimization/</guid><description>&lt;!-- raw HTML omitted -->
&lt;details class="toc-inpage d-print-none " open>
&lt;summary class="font-weight-bold">Table of Contents&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#part-1-content-reconstruction">Part 1: Content Reconstruction&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#experiments">Experiments&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#part-1-texture-synthesis">Part 1: Texture Synthesis&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#experiments-1">Experiments&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#part-2-style-transfer">Part 2: Style Transfer&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#experiments-2">Experiments&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;ul>
&lt;li>In this project, I started by optimizing random noise in content space, which helped me understand the concept of optimizing pixels based on specific losses.&lt;/li>
&lt;li>Then, I focused on generating textures by optimizing the style only, which allowed me to grasp the connection between style-space distance and the gram matrix.&lt;/li>
&lt;li>Finally, I combined all these elements to perform the Neural Style Transfer, creating a beautiful, Frida-Kahlo-inspired rendition of Fallingwater.&lt;/li>
&lt;/ul>
&lt;p>Feel free to explore the images below to see the original content image, the style image, and the final Neural Style Transfer output. Let your imagination run wild as you discover the endless possibilities of blending art and technology!&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h2 id="part-1-content-reconstruction">Part 1: Content Reconstruction&lt;/h2>
&lt;h3 id="experiments">Experiments&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Effect of optimizing content loss at different layers:&lt;/strong> Explored the impact of optimizing content loss at various layers and chose the best one.
&lt;figure id="figure-reconstruct-content-layer-1-to-16-top-left-16-bottom-right-1">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/reconstruct/content_16-1_style_1.png" alt="Reconstruct, Content Layer [1 to 16], Top Left (16), Bottom Right (1)" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Reconstruct, Content Layer [1 to 16], Top Left (16), Bottom Right (1)
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Comparison of two random noise input images:&lt;/strong> Optimized two random noise input images with content loss and compared their results with the content image.
&lt;figure id="figure-wally">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/reconstruct/content_wally.png" alt="Wally" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Wally
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-reconstruct-wally-content-layer-1">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/reconstruct/reconstruct_wally.png" alt="Reconstruct: Wally, Content Layer [1]" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Reconstruct: Wally, Content Layer [1]
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-falling-water">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/reconstruct/content_fallingwater.png" alt="Falling Water" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Falling Water
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-reconstruct-falling-water-content-layer1">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/reconstruct/reconstruct_fallingwater.png" alt="Reconstruct: Falling Water, Content Layer[1]" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Reconstruct: Falling Water, Content Layer[1]
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="part-1-texture-synthesis">Part 1: Texture Synthesis&lt;/h2>
&lt;p>In this project, we implemented a texture synthesis method using style-space loss, inspired by the Gram matrix. By measuring the distance between the styles of two images, we aimed to optimize and predict features that closely resemble the target style.&lt;/p>
&lt;h3 id="experiments-1">Experiments&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Effect of optimizing texture loss at different layers:&lt;/strong> Explored the impact of optimizing style loss at various layers and chose the best one. We discovered that the textures generated when optimizing style layers 1 to 5 exhibited the highest similarity with the original image. In contrast, the results became increasingly noisy and less visually coherent when the optimization was performed on later layers, such as layers 11 to 15. This observation suggests that earlier layers play a more significant role in capturing and reproducing the style features of the original image.
&lt;figure id="figure-synthesis-style-layer-1-5-to-11-15-top-left-11-15-bottom-right-1-5">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/synthesis/content_4_style_11-1.png" alt="Synthesis, Style Layer [1-5 to 11-15], Top Left (11-15), Bottom Right (1-5)" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Synthesis, Style Layer [1-5 to 11-15], Top Left (11-15), Bottom Right (1-5)
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Comparison of two random noise input images:&lt;/strong> Optimized two random noise input images with content loss and compared their results with the content image.
&lt;figure id="figure-frida-kahlo">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/synthesis/style_1.png" alt="Frida Kahlo" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Frida Kahlo
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-synthesis-frida-kahlo-style-layer-1-5">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/synthesis/synthesis_content_4_style_1_4.png" alt="Synthesis: Frida Kahlo, Style Layer [1-5]" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Synthesis: Frida Kahlo, Style Layer [1-5]
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-picasso">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/synthesis/style_2.png" alt="Picasso" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Picasso
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-synthesis-picasso-style-layer-1-5">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/synthesis/synthesis_content_4_style_1_4_picasso.png" alt="Synthesis: Picasso, Style Layer [1-5]" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Synthesis: Picasso, Style Layer [1-5]
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="part-2-style-transfer">Part 2: Style Transfer&lt;/h2>
&lt;p>In the final part of this project, we combined content and style loss to perform style transfer. By applying both losses to specific layers, we were able to generate stylized images that maintain the content of the original image while adopting the style of a reference image.&lt;/p>
&lt;h3 id="experiments-2">Experiments&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Hyper-parameter tuning:&lt;/strong> We carefully tuned the hyper-parameters to achieve satisfactory resultsã€‚We ran two for loops, one to traverse content from [1 to 16] and another one to traverse style [1-5 to 11-15]. Each row uses a fixed content layer, each column shares a fixed style layer.
&lt;figure id="figure-transfer-content-layer-from-15-13">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/15-13.png" alt="Transfer, content layer from 15-13" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Transfer, content layer from 15-13
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-transfer-content-layer-from-12-10">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/12-10.png" alt="Transfer, content layer from 12-10" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Transfer, content layer from 12-10
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-transfer-content-layer-from-9-7">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/9-7.png" alt="Transfer, content layer from 9-7" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Transfer, content layer from 9-7
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-transfer-content-layer-from-6-4">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/6-4.png" alt="Transfer, content layer from 6-4" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Transfer, content layer from 6-4
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-transfer-content-layer-from-3-1">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/3-1.png" alt="Transfer, content layer from 3-1" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Transfer, content layer from 3-1
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Gram Matrix Implementation:&lt;/strong> The Gram matrix is a crucial component in style transfer, as it helps capture and quantify the style of an image. It works by computing the correlation between different feature maps in a given layer of a neural network, thus providing a representation of the style information contained in that layer.&lt;/p>
&lt;figure id="figure-gram-matrix-source-from-cloudxlabcom">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/gram_matrix.png" alt="Gram Matrix, source from cloudxlab.com " loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Gram Matrix, source from cloudxlab.com
&lt;/figcaption>&lt;/figure>
&lt;!-- raw HTML omitted -->
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Image Grid:&lt;/strong> We generated a grid of images, showcasing the results of style transfer with two content images mixed with two style images. The grid also includes the original content and style images.
&lt;figure id="figure-content-images">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/content2.png" alt="Content Images" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Content Images
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-style-images">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/style_2.png" alt="Style Images" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Style Images
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-style-transfer-mixed-2-by-2">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/2by2Grid.png" alt="Style Transfer: Mixed 2 By 2" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Style Transfer: Mixed 2 By 2
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Style Transfer on My Favorite Image:&lt;/strong> We applied style transfer to some of our favorite images and observed the results.
&lt;figure id="figure-style-untitled-beauty-products-by-andy-warhol">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/style_andy.png" alt="Style: Untitled (Beauty Products) by Andy Warhol" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Style: Untitled (Beauty Products) by Andy Warhol
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-content-laguna-beach-by-linji-wang">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/content_laguna_beach.png" alt="Content: Laguna Beach by Linji Wang" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Content: Laguna Beach by Linji Wang
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-andy-warhol-styled-laguna-beach">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/style_laguna_beach.png" alt="Andy Warhol Styled Laguna Beach" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Andy Warhol Styled Laguna Beach
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-style-transfer-process">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/transfer/untitled-beauty-productsLarge_laguna_beach_2023-04-03-02-04-02.gif" alt="Style Transfer Process" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Style Transfer Process
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In conclusion, this project has successfully demonstrated the implementation of neural style transfer using content and style losses. The assignment began with content reconstruction, where the content loss was calculated as the squared L2-distance between the features of the input and content images at a certain layer. Different layers were evaluated for their effect on content optimization, and the results were analyzed and presented.&lt;/p>
&lt;p>The second part of the assignment focused on texture synthesis using style loss, which was computed based on the Gram matrices of the input and style images. The effect of optimizing texture loss at different layers was explored, and synthesized textures were generated and compared.&lt;/p>
&lt;p>Finally, both content and style losses were integrated to perform neural style transfer. Hyperparameters were tuned, and a 3x3 grid of results, including content and style images, was generated. The quality and running time of the style transfer were compared when using random noise and a content image as input. Furthermore, the style transfer technique was applied to a variety of favorite images, showcasing its versatility.&lt;/p>
&lt;p>Overall, this project has deepened the understanding of neural style transfer, optimization of pixel values, and the role of content and style losses in producing visually appealing and artistic results. The experiments carried out and the results obtained provide valuable insights into the workings of neural style transfer and its potential applications in the creative domain.&lt;/p></description></item><item><title>When Cats meet GANs</title><link>https://linjiw.github.io/post/gan/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://linjiw.github.io/post/gan/</guid><description>&lt;!-- raw HTML omitted -->
&lt;details class="toc-inpage d-print-none " open>
&lt;summary class="font-weight-bold">Table of Contents&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#part-1-deep-convolutional-gan">Part 1: Deep Convolutional GAN&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#experiment-with-dcgans">Experiment with DCGANs&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#part-2-cyclegan">Part 2: CycleGAN&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#data-augmentation">Data Augmentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#generator">Generator&lt;/a>&lt;/li>
&lt;li>&lt;a href="#experiment-with-cyclegan">Experiment with CycleGAN&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#bells--whistles">Bells &amp;amp; Whistles&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#implement-and-train-a-diffusion-model">Implement and train a diffusion model&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion-1">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In this assignment, we get hands-on experience coding and training GANs. This assignment includes two parts:&lt;/p>
&lt;p>Implementing a Deep Convolutional GAN (DCGAN) to generate grumpy cats from samples of random noise.
Implementing a more complex GAN architecture called CycleGAN for the task of image-to-image translation. We train the CycleGAN to convert between different types of two kinds of cats (Grumpy and Russian Blue) and between apples and oranges.&lt;/p>
&lt;h2 id="part-1-deep-convolutional-gan">Part 1: Deep Convolutional GAN&lt;/h2>
&lt;p>For the first part of this assignment, we implement a slightly modified version of Deep Convolutional GAN (DCGAN).&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h3 id="experiment-with-dcgans">Experiment with DCGANs&lt;/h3>
&lt;p>We&amp;rsquo;ve been experimenting with different data preprocessing techniques, and we&amp;rsquo;ve found that the choice of preprocessing can have a significant impact on the performance of the GAN. To demonstrate this, we&amp;rsquo;ve included screenshots of the training loss for both the discriminator and generator with two different preprocessing options: basic, deluxe and diff_aug.&lt;/p>
&lt;h4 id="grumpifybprocessed_basic">grumpifyBprocessed_basic&lt;/h4>
&lt;p>
&lt;figure id="figure-sample-data_preprocessbasic-iter--6400">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_basic/sample-006400.png" alt="sample: data_preprocess=basic, iter = 6400" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
sample: data_preprocess=basic, iter = 6400
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-d_fake_loss-data_preprocessbasic-iter--6400">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_basic/D_fake_loss.png" alt="D_fake_loss: data_preprocess=basic, iter = 6400" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
D_fake_loss: data_preprocess=basic, iter = 6400
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-d_real_loss-data_preprocessbasic-iter--6400">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_basic/D_real_loss.png" alt="D_real_loss: data_preprocess=basic, iter = 6400" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
D_real_loss: data_preprocess=basic, iter = 6400
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-d_total_loss-data_preprocessbasic-iter--6400">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_basic/D_total_loss.png" alt="D_total_loss: data_preprocess=basic, iter = 6400" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
D_total_loss: data_preprocess=basic, iter = 6400
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-g_loss-data_preprocessbasic-iter--6400">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_basic/G_loss.png" alt="G_loss: data_preprocess=basic, iter = 6400" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
G_loss: data_preprocess=basic, iter = 6400
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h4 id="grumpifybprocessed_deluxe">grumpifyBprocessed_deluxe&lt;/h4>
&lt;p>
&lt;figure id="figure-data_preprocessdeluxe-iter--6400">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_deluxe/sample-006400.png" alt="data_preprocess=deluxe, iter = 6400" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
data_preprocess=deluxe, iter = 6400
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-d_fake_loss-data_preprocessdeluxe-iter--6400">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_deluxe/D_fake_loss.png" alt="D_fake_loss: data_preprocess=deluxe, iter = 6400" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
D_fake_loss: data_preprocess=deluxe, iter = 6400
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-d_real_loss-data_preprocessdeluxe-iter--6400">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_deluxe/D_real_loss.png" alt="D_real_loss: data_preprocess=deluxe, iter = 6400" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
D_real_loss: data_preprocess=deluxe, iter = 6400
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-d_total_loss-data_preprocessdeluxe-iter--6400">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_deluxe/D_total_loss.png" alt="D_total_loss: data_preprocess=deluxe, iter = 6400" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
D_total_loss: data_preprocess=deluxe, iter = 6400
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-g_loss-data_preprocessdeluxe-iter--6400">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_deluxe/G_loss.png" alt="G_loss: data_preprocess=deluxe, iter = 6400" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
G_loss: data_preprocess=deluxe, iter = 6400
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-data_preprocessdeluxe-iter--6400-diff_aug--true">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_deluxe_diffaug/sample-006400.png" alt="data_preprocess=deluxe, iter = 6400, diff_aug = True" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
data_preprocess=deluxe, iter = 6400, diff_aug = True
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h4 id="grumpifybprocessed_deluxe_diffaug">grumpifyBprocessed_deluxe_diffaug&lt;/h4>
&lt;p>
&lt;figure id="figure-d_fake_loss-data_preprocessdeluxe-iter--6400-diff_aug--true">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_deluxe_diffaug/D_fake_loss.png" alt="D_fake_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
D_fake_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-d_real_loss-data_preprocessdeluxe-iter--6400-diff_aug--true">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_deluxe_diffaug/D_real_loss.png" alt="D_real_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
D_real_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-d_total_loss-data_preprocessdeluxe-iter--6400-diff_aug--true">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_deluxe_diffaug/D_total_loss.png" alt="D_total_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
D_total_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-g_loss-data_preprocessdeluxe-iter--6400-diff_aug--true">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/grumpifyBprocessed_deluxe_diffaug/G_loss.png" alt="G_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
G_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h4 id="results-analysis">Results analysis&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Data Preprocessing&lt;/th>
&lt;th>Discriminator Loss&lt;/th>
&lt;th>Generator Loss&lt;/th>
&lt;th>Convergence Rate&lt;/th>
&lt;th>Stability&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Basic&lt;/td>
&lt;td>Slow decrease, potential instability&lt;/td>
&lt;td>Fluctuates, struggles to generate realistic images&lt;/td>
&lt;td>Slow&lt;/td>
&lt;td>Less stable&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Deluxe&lt;/td>
&lt;td>Faster decrease, more effective at differentiation&lt;/td>
&lt;td>Converges more quickly, learns from more varied examples&lt;/td>
&lt;td>Faster&lt;/td>
&lt;td>More stable&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Differential Augmentations&lt;/td>
&lt;td>Even faster decrease, more effective at differentiation&lt;/td>
&lt;td>Faster generation of diverse and realistic images&lt;/td>
&lt;td>Fastest&lt;/td>
&lt;td>Most stable&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The table above highlights the key differences in the loss curves for a DCGAN trained with different data preprocessing techniques. Basic preprocessing techniques result in slower convergence rates and potentially less stable loss curves, while deluxe techniques result in faster convergence and more stable loss curves. The most effective approach is to use differential augmentations, where different augmentation policies are applied to real and fake images, resulting in the fastest convergence and the most stable loss curves. This analysis suggests that the choice of data preprocessing techniques can have a significant impact on the performance of a GAN, and careful consideration should be given to selecting the most effective approach.&lt;/p>
&lt;h2 id="part-2-cyclegan">Part 2: CycleGAN&lt;/h2>
&lt;p>Implemented the CycleGAN architecture.&lt;/p>
&lt;h3 id="data-augmentation">Data Augmentation&lt;/h3>
&lt;p>Set the &amp;ndash;data_preprocess flag to deluxe.&lt;/p>
&lt;h3 id="generator">Generator&lt;/h3>
&lt;p>Implemented the generator architecture by completing the &lt;strong>init&lt;/strong> method of the CycleGenerator class in models.py.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h3 id="experiment-with-cyclegan">Experiment with CycleGAN&lt;/h3>
&lt;!-- raw HTML omitted -->
&lt;h4 id="cat_10deluxe_instance_dc_cycle_naive">cat_10deluxe_instance_dc_cycle_naive&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Title&lt;/th>
&lt;th style="text-align:center">Image&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">sample X to Y&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_dc_cycle_naive/sample-001000-X-Y.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">sample Y to X&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_dc_cycle_naive/sample-001000-Y-X.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_fake_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_dc_cycle_naive/D_fake_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_real_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_dc_cycle_naive/D_real_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_X_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_dc_cycle_naive/D_X_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_Y_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_dc_cycle_naive/D_Y_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">G_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_dc_cycle_naive/G_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- raw HTML omitted -->
&lt;h4 id="cat_10deluxe_instance_patch_cycle_naive">cat_10deluxe_instance_patch_cycle_naive&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Title&lt;/th>
&lt;th style="text-align:center">Image&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">sample X to Y&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive/sample-001000-X-Y.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">sample Y to X&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive/sample-001000-Y-X.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_fake_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive/D_fake_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_real_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive/D_real_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_X_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive/D_X_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_Y_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive/D_Y_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">G_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive/G_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- raw HTML omitted -->
&lt;h4 id="cat_10deluxe_instance_patch_cycle_naive_cycle">cat_10deluxe_instance_patch_cycle_naive_cycle&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Title&lt;/th>
&lt;th style="text-align:center">Image&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">sample X to Y&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle/sample-001000-X-Y.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">sample Y to X&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle/sample-001000-Y-X.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_fake_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle/D_fake_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_real_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle/D_real_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_X_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle/D_X_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_Y_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle/D_Y_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">G_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle/G_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- raw HTML omitted -->
&lt;h4 id="cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug">cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Title&lt;/th>
&lt;th style="text-align:center">Image&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">sample X to Y&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug/sample-010000-X-Y.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">sample Y to X&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug/sample-010000-Y-X.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_fake_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug/D_fake_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_real_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug/D_real_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_X_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug/D_X_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_Y_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug/D_Y_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">G_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug/G_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h4 id="apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug">apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Title&lt;/th>
&lt;th style="text-align:center">Image&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">sample X to Y&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug/sample-010000-X-Y.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">sample Y to X&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug/sample-010000-Y-X.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_fake_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug/D_fake_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_real_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug/D_real_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_X_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug/D_X_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">D_Y_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug/D_Y_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">G_loss&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug/G_loss.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!-- raw HTML omitted -->
&lt;h4 id="observations">Observations:&lt;/h4>
&lt;p>We observed that the results with the cycle-consistency loss were better than the results without it. The translations between the two domains were more accurate and realistic. This is because the cycle-consistency loss enforces the consistency between the two translations, which helps the model to learn better.&lt;/p>
&lt;p>We also observed that the DCDiscriminator resulted in better quality translations than the PatchDiscriminator. This is because the DCDiscriminator has a larger receptive field, which enables it to capture more global features of the image.&lt;/p>
&lt;h4 id="conclusion">Conclusion:&lt;/h4>
&lt;p>In conclusion, we have trained CycleGAN from scratch with and without the cycle-consistency loss, and have compared the results using the DCDiscriminator and the PatchDiscriminator. We have observed that the cycle-consistency loss and the DCDiscriminator resulted in better quality translations between the two domains. These observations can help in improving the translation quality between different domains in image processing applications.&lt;/p>
&lt;h2 id="bells--whistles">Bells &amp;amp; Whistles&lt;/h2>
&lt;h3 id="implement-and-train-a-diffusion-model">Implement and train a diffusion model&lt;/h3>
&lt;h4 id="training-diffusion-models-with-hugging-faces-diffusers">Training Diffusion Models with Hugging Face&amp;rsquo;s Diffusers&lt;/h4>
&lt;h4 id="introduction-1">Introduction&lt;/h4>
&lt;p>In this project, we train a simple diffusion model using the Hugging Face&amp;rsquo;s Diffusers library. Diffusion models have become state-of-the-art generative models in recent times.&lt;/p>
&lt;h4 id="key-parts-of-the-code">Key Parts of the Code&lt;/h4>
&lt;h5 id="configuration">Configuration:&lt;/h5>
&lt;p>We define a &amp;lsquo;TrainingConfig&amp;rsquo; class that holds all the training hyperparameters.
Hyperparameters include &amp;lsquo;image_size&amp;rsquo;, &amp;rsquo;train_batch_size&amp;rsquo;, &amp;rsquo;eval_batch_size&amp;rsquo;, &amp;rsquo;num_epochs&amp;rsquo;, &amp;lsquo;gradient_accumulation_steps&amp;rsquo;, &amp;rsquo;learning_rate&amp;rsquo;, and &amp;rsquo;lr_warmup_steps&amp;rsquo;, among others.&lt;/p>
&lt;h5 id="data-preprocessing">Data Preprocessing:&lt;/h5>
&lt;p>We use the datasets library to load our dataset and apply data transformations.
The dataset is preprocessed using the transforms.Compose function from torchvision.
The dataset is then transformed on-the-fly during training.&lt;/p>
&lt;h5 id="model-definition">Model Definition:&lt;/h5>
&lt;p>We define our model using the &amp;lsquo;UNet2DModel&amp;rsquo; class from the diffusers library.
The model has various hyperparameters such as &amp;lsquo;sample_size&amp;rsquo;, &amp;lsquo;in_channels&amp;rsquo;, &amp;lsquo;out_channels&amp;rsquo;, &amp;rsquo;layers_per_block&amp;rsquo;, &amp;lsquo;block_out_channels&amp;rsquo;, &amp;lsquo;down_block_types&amp;rsquo;, and &amp;lsquo;up_block_types&amp;rsquo;.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h4 id="training-setup">Training Setup:&lt;/h4>
&lt;p>We use an AdamW optimizer and a cosine learning rate schedule for training.
We use the DDPMPipeline class from the diffusers library for end-to-end inference during evaluation.
The training function train_loop is defined, which includes gradient accumulation, mixed precision training, and multi-GPU or TPU training using the Accelerator class from the accelerate library.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>We use the &amp;rsquo;notebook_launcher&amp;rsquo; function from the accelerate library to launch the training from the notebook.&lt;/p>
&lt;h4 id="key-functions">Key Functions&lt;/h4>
&lt;p>&lt;em>transform(examples)&lt;/em>: Applies the image transformations on the fly during training.
&lt;em>evaluate(config, epoch, pipeline)&lt;/em>: Generates a batch of sample images during evaluation and saves them as a grid to the disk.
&lt;em>train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)&lt;/em>: The main training loop, which includes the forward diffusion process, loss calculation, and backpropagation.&lt;/p>
&lt;h4 id="diffusion-results">Diffusion Results&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Title&lt;/th>
&lt;th style="text-align:center">Image&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">Apple&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/difussion/apple.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">Cat&lt;/td>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./data/difussion/cat.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The quality of the generated images and how well the DCGAN has captured the main differences between the two domains depend on factors such as the quality of the training data, hyperparameters used during training, and complexity of image domains. If the diffusion results look unrealistic compared to the DCGAN results, it could be due to factors such as dataset quality, model complexity, hyperparameter tuning, or training time. Further analysis and experimentation would be necessary to pinpoint the specific reason for the difference in image quality.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h2 id="conclusion-1">Conclusion&lt;/h2>
&lt;p>This report presents our implementation of DCGAN and CycleGAN for various image generation tasks. Through these experiments, we have observed the impact of data augmentation and differentiable augmentation on the training process and final results. We have also seen the capabilities of CycleGAN in generating realistic images for domain-to-domain translation tasks, such as converting Grumpy cats to Russian Blue cats and vice versa, and converting apples to oranges and vice versa.&lt;/p></description></item><item><title>When Cats meet GANs</title><link>https://linjiw.github.io/project/gan/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://linjiw.github.io/project/gan/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In this assignment, we get hands-on experience coding and training GANs. This assignment includes two parts:&lt;/p>
&lt;p>Implementing a Deep Convolutional GAN (DCGAN) to generate grumpy cats from samples of random noise.
Implementing a more complex GAN architecture called CycleGAN for the task of image-to-image translation. We train the CycleGAN to convert between different types of two kinds of cats (Grumpy and Russian Blue) and between apples and oranges.&lt;/p>
&lt;h2 id="part-1-deep-convolutional-gan">Part 1: Deep Convolutional GAN&lt;/h2>
&lt;p>For the first part of this assignment, we implement a slightly modified version of Deep Convolutional GAN (DCGAN).&lt;/p>
&lt;h3 id="implement-data-augmentation">Implement Data Augmentation&lt;/h3>
&lt;p>Implemented the deluxe version of data augmentation in &amp;lsquo;data_loader.py&amp;rsquo;.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">elif&lt;/span> opts&lt;span style="color:#f92672">.&lt;/span>data_preprocess &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#39;deluxe&amp;#39;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> load_size &lt;span style="color:#f92672">=&lt;/span> int(&lt;span style="color:#ae81ff">1.1&lt;/span> &lt;span style="color:#f92672">*&lt;/span> opts&lt;span style="color:#f92672">.&lt;/span>image_size)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> osize &lt;span style="color:#f92672">=&lt;/span> [load_size, load_size]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> deluxe_transform &lt;span style="color:#f92672">=&lt;/span> transforms&lt;span style="color:#f92672">.&lt;/span>Compose([
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> transforms&lt;span style="color:#f92672">.&lt;/span>Resize(opts&lt;span style="color:#f92672">.&lt;/span>image_size, Image&lt;span style="color:#f92672">.&lt;/span>BICUBIC),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> transforms&lt;span style="color:#f92672">.&lt;/span>RandomCrop(opts&lt;span style="color:#f92672">.&lt;/span>image_size),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> transforms&lt;span style="color:#f92672">.&lt;/span>RandomHorizontalFlip(),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> transforms&lt;span style="color:#f92672">.&lt;/span>ToTensor(),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> transforms&lt;span style="color:#f92672">.&lt;/span>Normalize((&lt;span style="color:#ae81ff">0.5&lt;/span>, &lt;span style="color:#ae81ff">0.5&lt;/span>, &lt;span style="color:#ae81ff">0.5&lt;/span>), (&lt;span style="color:#ae81ff">0.5&lt;/span>, &lt;span style="color:#ae81ff">0.5&lt;/span>, &lt;span style="color:#ae81ff">0.5&lt;/span>)),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ])
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> train_transform &lt;span style="color:#f92672">=&lt;/span> deluxe_transform
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">pass&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="implement-the-discriminator-of-the-dcgan">Implement the Discriminator of the DCGAN&lt;/h3>
&lt;p>(Answer for padding calculation goes here)&lt;/p>
&lt;p>Implemented the architecture by filling in the &amp;lsquo;&lt;strong>init&lt;/strong>&amp;rsquo; and &amp;lsquo;forward&amp;rsquo; method of the &amp;lsquo;DCDiscriminator&amp;rsquo; class in &amp;lsquo;models.py&amp;rsquo;.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> __init__(self, conv_dim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">64&lt;/span>, norm&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;instance&amp;#39;&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> super()&lt;span style="color:#f92672">.&lt;/span>__init__()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>conv1 &lt;span style="color:#f92672">=&lt;/span> conv(&lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">32&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, norm, &lt;span style="color:#66d9ef">False&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>conv2 &lt;span style="color:#f92672">=&lt;/span> conv(&lt;span style="color:#ae81ff">32&lt;/span>, &lt;span style="color:#ae81ff">64&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, norm, &lt;span style="color:#66d9ef">False&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>conv3 &lt;span style="color:#f92672">=&lt;/span> conv(&lt;span style="color:#ae81ff">64&lt;/span>, &lt;span style="color:#ae81ff">128&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, norm, &lt;span style="color:#66d9ef">False&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>conv4 &lt;span style="color:#f92672">=&lt;/span> conv(&lt;span style="color:#ae81ff">128&lt;/span>, &lt;span style="color:#ae81ff">256&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, norm, &lt;span style="color:#66d9ef">False&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>conv5 &lt;span style="color:#f92672">=&lt;/span> conv(&lt;span style="color:#ae81ff">256&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#66d9ef">None&lt;/span>, &lt;span style="color:#66d9ef">False&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">forward&lt;/span>(self, x):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;Forward pass, x is (B, C, H, W).&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>conv1(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>conv2(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>conv3(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>conv4(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>conv5(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> x&lt;span style="color:#f92672">.&lt;/span>squeeze()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generator">Generator&lt;/h3>
&lt;p>Implemented the generator of the DCGAN by filling in the &amp;lsquo;&lt;strong>init&lt;/strong>&amp;rsquo; and &amp;lsquo;forward&amp;rsquo; method of the &amp;lsquo;DCGenerator&amp;rsquo; class in &amp;lsquo;models.py&amp;rsquo;.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> __init__(self, noise_size, conv_dim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">64&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> super()&lt;span style="color:#f92672">.&lt;/span>__init__()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>up_conv1 &lt;span style="color:#f92672">=&lt;/span> conv(&lt;span style="color:#ae81ff">100&lt;/span>, &lt;span style="color:#ae81ff">256&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;instance&amp;#39;&lt;/span>, &lt;span style="color:#66d9ef">False&lt;/span>,&lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>up_conv2 &lt;span style="color:#f92672">=&lt;/span> up_conv(&lt;span style="color:#ae81ff">256&lt;/span>, &lt;span style="color:#ae81ff">128&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, stride&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, padding&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, scale_factor&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, norm&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;instance&amp;#39;&lt;/span>, activ&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>up_conv3 &lt;span style="color:#f92672">=&lt;/span> up_conv(&lt;span style="color:#ae81ff">128&lt;/span>, &lt;span style="color:#ae81ff">64&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, stride&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, padding&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, scale_factor&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, norm&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;instance&amp;#39;&lt;/span>, activ&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>up_conv4 &lt;span style="color:#f92672">=&lt;/span> up_conv(&lt;span style="color:#ae81ff">64&lt;/span>, &lt;span style="color:#ae81ff">32&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, stride&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, padding&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, scale_factor&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, norm&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;instance&amp;#39;&lt;/span>, activ&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>up_conv5 &lt;span style="color:#f92672">=&lt;/span> up_conv(&lt;span style="color:#ae81ff">32&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, stride&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, padding&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, scale_factor&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, norm&lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>, activ&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;tanh&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">forward&lt;/span>(self, z):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> Generate an image given a sample of random noise.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> Input
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> -----
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> z: BS x noise_size x 1 x 1 --&amp;gt; 16x100x1x1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> Output
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> ------
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> out: BS x channels x image_width x image_height --&amp;gt; 16x3x64x64
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> z &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>up_conv1(z)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> z &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>up_conv2(z)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> z &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>up_conv3(z)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> z &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>up_conv4(z)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> z &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>up_conv5(z)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> z
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="training-loop">Training Loop&lt;/h3>
&lt;p>Implemented the training loop for the DCGAN by filling in the indicated parts of the training_loop function in vanilla_gan.py.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># TRAIN THE DISCRIMINATOR&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 1. Compute the discriminator loss on real images&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> opts&lt;span style="color:#f92672">.&lt;/span>use_diffaug:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> D_real_loss &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>mean((D(DiffAugment(real_images, policy&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;color,translation,cutout&amp;#39;&lt;/span>, channels_first&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span> )) &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>) &lt;span style="color:#f92672">**&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> D_real_loss &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>mean((D(real_images) &lt;span style="color:#f92672">-&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>) &lt;span style="color:#f92672">**&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 2. Sample noise&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> noise &lt;span style="color:#f92672">=&lt;/span> sample_noise(opts&lt;span style="color:#f92672">.&lt;/span>batch_size, opts&lt;span style="color:#f92672">.&lt;/span>noise_size)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 3. Generate fake images from the noise&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fake_images &lt;span style="color:#f92672">=&lt;/span> G(noise)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 4. Compute the discriminator loss on the fake images&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> opts&lt;span style="color:#f92672">.&lt;/span>use_diffaug:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> D_fake_loss &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>mean((D(DiffAugment(fake_images&lt;span style="color:#f92672">.&lt;/span>detach(), policy&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;color,translation,cutout&amp;#39;&lt;/span>, channels_first&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span> ))) &lt;span style="color:#f92672">**&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> D_real_loss &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>mean((D(fake_images&lt;span style="color:#f92672">.&lt;/span>detach())) &lt;span style="color:#f92672">**&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> D_total_loss &lt;span style="color:#f92672">=&lt;/span> (D_real_loss &lt;span style="color:#f92672">+&lt;/span> D_fake_loss) &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># update the discriminator D&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> d_optimizer&lt;span style="color:#f92672">.&lt;/span>zero_grad()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> D_total_loss&lt;span style="color:#f92672">.&lt;/span>backward()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> d_optimizer&lt;span style="color:#f92672">.&lt;/span>step()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># TRAIN THE GENERATOR&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 1. Sample noise&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> noise &lt;span style="color:#f92672">=&lt;/span> sample_noise(opts&lt;span style="color:#f92672">.&lt;/span>batch_size, opts&lt;span style="color:#f92672">.&lt;/span>noise_size)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 2. Generate fake images from the noise&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fake_images &lt;span style="color:#f92672">=&lt;/span> G(noise)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># 3. Compute the generator loss&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> opts&lt;span style="color:#f92672">.&lt;/span>use_diffaug:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> G_loss &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>mean((D(DiffAugment(fake_images, policy&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;color,translation,cutout&amp;#39;&lt;/span>, channels_first&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span> ))&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>) &lt;span style="color:#f92672">**&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">else&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> G_loss &lt;span style="color:#f92672">=&lt;/span> torch&lt;span style="color:#f92672">.&lt;/span>mean((D(fake_images)&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>) &lt;span style="color:#f92672">**&lt;/span> &lt;span style="color:#ae81ff">2&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="differentiable-augmentation">Differentiable Augmentation&lt;/h4>
&lt;p>(Discussion of results with and without applying differentiable augmentations, and the difference between two augmentation schemes in terms of implementation and effects)&lt;/p>
&lt;h3 id="experiment-with-dcgans">Experiment with DCGANs&lt;/h3>
&lt;p>INSERT IMAGE: Screenshots of discriminator and generator training loss with &amp;ndash;data_preprocess=basic, &amp;ndash;data_preprocess=deluxe.&lt;/p>
&lt;p>(Brief explanation of what the curves should look like if GAN manages to train)&lt;/p>
&lt;p>INSERT IMAGE: With &amp;ndash;data_preprocess=deluxe and differentiable augmentation enabled, show one of the samples from early in training (e.g., iteration 200) and one of the samples from later in training, and give the iteration number for those samples.&lt;/p>
&lt;p>(Brief comment on the quality of the samples, and in what way they improve through training)&lt;/p>
&lt;h2 id="part-2-cyclegan">Part 2: CycleGAN&lt;/h2>
&lt;p>Implemented the CycleGAN architecture.&lt;/p>
&lt;h3 id="data-augmentation">Data Augmentation&lt;/h3>
&lt;p>Set the &amp;ndash;data_preprocess flag to deluxe.&lt;/p>
&lt;h3 id="generator-1">Generator&lt;/h3>
&lt;p>Implemented the generator architecture by completing the &lt;strong>init&lt;/strong> method of the CycleGenerator class in models.py.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> __init__(self, conv_dim&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">64&lt;/span>, init_zero_weights&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">False&lt;/span>, norm&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;instance&amp;#39;&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> super()&lt;span style="color:#f92672">.&lt;/span>__init__()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># # 1. Define the encoder part of the generator&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>conv1 &lt;span style="color:#f92672">=&lt;/span> conv(&lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">32&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, norm, &lt;span style="color:#66d9ef">False&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>conv2 &lt;span style="color:#f92672">=&lt;/span> conv(&lt;span style="color:#ae81ff">32&lt;/span>, &lt;span style="color:#ae81ff">64&lt;/span>, &lt;span style="color:#ae81ff">4&lt;/span>, &lt;span style="color:#ae81ff">2&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, norm, &lt;span style="color:#66d9ef">False&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># # 2. Define the transformation part of the generator&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>resnet_block &lt;span style="color:#f92672">=&lt;/span> nn&lt;span style="color:#f92672">.&lt;/span>Sequential(ResnetBlock(conv_dim &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">64&lt;/span>, norm &lt;span style="color:#f92672">=&lt;/span> norm, activ &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ResnetBlock(conv_dim &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">64&lt;/span>, norm &lt;span style="color:#f92672">=&lt;/span> norm, activ &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ResnetBlock(conv_dim &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">64&lt;/span>, norm &lt;span style="color:#f92672">=&lt;/span> norm, activ &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>),)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># # 3. Define the decoder part of the generator&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>up_conv1 &lt;span style="color:#f92672">=&lt;/span> up_conv(&lt;span style="color:#ae81ff">64&lt;/span>, &lt;span style="color:#ae81ff">32&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, stride&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, padding&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, scale_factor&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, norm&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;instance&amp;#39;&lt;/span>, activ&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;relu&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#f92672">.&lt;/span>up_conv2 &lt;span style="color:#f92672">=&lt;/span> up_conv(&lt;span style="color:#ae81ff">32&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, &lt;span style="color:#ae81ff">3&lt;/span>, stride&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, padding&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>, scale_factor&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">2&lt;/span>, norm&lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">None&lt;/span>, activ&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;tanh&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">forward&lt;/span>(self, x):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> Generate an image conditioned on an input image.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> Input
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> -----
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> x: BS x 3 x 32 x 32
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> Output
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> ------
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> out: BS x 3 x 32 x 32
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>conv1(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>conv2(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>resnet_block(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>up_conv1(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">=&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>up_conv2(x)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> x
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="training-loop-1">Training Loop&lt;/h3>
&lt;p>Implemented the training loop for the CycleGAN by filling in the indicated parts of the training_loop function in cycle_gan.py.&lt;/p>
&lt;h3 id="experiment-with-cyclegan">Experiment with CycleGAN&lt;/h3>
&lt;p>INSERT IMAGE: Two example images of generated Grumpy cats from Russian Blue cats, and two example images of generated Russian Blue cats from Grumpy cats.&lt;/p>
&lt;p>(Brief comment on the quality of the generated images, and whether the CycleGAN has captured the main differences between the two domains)&lt;/p>
&lt;p>INSERT IMAGE: Two example images of generated apples from oranges, and two example images of generated oranges from apples.&lt;/p>
&lt;p>(Brief comment on the quality of the generated images, and whether the CycleGAN has captured the main differences between the two domains)&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>This report presents our implementation of DCGAN and CycleGAN for various image generation tasks. Through these experiments, we have observed the impact of data augmentation and differentiable augmentation on the training process and final results. We have also seen the capabilities of CycleGAN in generating realistic images for domain-to-domain translation tasks, such as converting Grumpy cats to Russian Blue cats and vice versa, and converting apples to oranges and vice versa.&lt;/p></description></item></channel></rss>