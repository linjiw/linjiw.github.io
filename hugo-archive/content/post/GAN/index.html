<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="introduction">Introduction</h2> <p>In this assignment, we get hands-on experience coding and training GANs. This assignment includes two parts:</p> <p>Implementing a Deep Convolutional GAN (DCGAN) to generate grumpy cats from samples of random noise. Implementing a more complex GAN architecture called CycleGAN for the task of image-to-image translation. We train the CycleGAN to convert between different types of two kinds of cats (Grumpy and Russian Blue) and between apples and oranges.</p> <h2 id="part-1-deep-convolutional-gan">Part 1: Deep Convolutional GAN</h2> <p>For the first part of this assignment, we implement a slightly modified version of Deep Convolutional GAN (DCGAN).</p> <p>–&gt;</p> <h3 id="experiment-with-dcgans">Experiment with DCGANs</h3> <p>We’ve been experimenting with different data preprocessing techniques, and we’ve found that the choice of preprocessing can have a significant impact on the performance of the GAN. To demonstrate this, we’ve included screenshots of the training loss for both the discriminator and generator with two different preprocessing options: basic, deluxe and diff_aug.</p> <h4 id="grumpifybprocessed_basic">grumpifyBprocessed_basic</h4> <h4 id="grumpifybprocessed_deluxe">grumpifyBprocessed_deluxe</h4> <h4 id="grumpifybprocessed_deluxe_diffaug">grumpifyBprocessed_deluxe_diffaug</h4> <h4 id="results-analysis">Results analysis</h4> <table> <thead> <tr> <th>Data Preprocessing</th> <th>Discriminator Loss</th> <th>Generator Loss</th> <th>Convergence Rate</th> <th>Stability</th> </tr> </thead> <tbody> <tr> <td>Basic</td> <td>Slow decrease, potential instability</td> <td>Fluctuates, struggles to generate realistic images</td> <td>Slow</td> <td>Less stable</td> </tr> <tr> <td>Deluxe</td> <td>Faster decrease, more effective at differentiation</td> <td>Converges more quickly, learns from more varied examples</td> <td>Faster</td> <td>More stable</td> </tr> <tr> <td>Differential Augmentations</td> <td>Even faster decrease, more effective at differentiation</td> <td>Faster generation of diverse and realistic images</td> <td>Fastest</td> <td>Most stable</td> </tr> </tbody> </table> <p>The table above highlights the key differences in the loss curves for a DCGAN trained with different data preprocessing techniques. Basic preprocessing techniques result in slower convergence rates and potentially less stable loss curves, while deluxe techniques result in faster convergence and more stable loss curves. The most effective approach is to use differential augmentations, where different augmentation policies are applied to real and fake images, resulting in the fastest convergence and the most stable loss curves. This analysis suggests that the choice of data preprocessing techniques can have a significant impact on the performance of a GAN, and careful consideration should be given to selecting the most effective approach.</p> <h2 id="part-2-cyclegan">Part 2: CycleGAN</h2> <p>Implemented the CycleGAN architecture.</p> <h3 id="data-augmentation">Data Augmentation</h3> <p>Set the –data_preprocess flag to deluxe.</p> <h3 id="generator">Generator</h3> <p>Implemented the generator architecture by completing the <strong>init</strong> method of the CycleGenerator class in models.py.</p> <h3 id="experiment-with-cyclegan">Experiment with CycleGAN</h3> <h4 id="cat_10deluxe_instance_dc_cycle_naive">cat_10deluxe_instance_dc_cycle_naive</h4> <p>| Title | Image | | :——: | :——: | |sample X to Y | | |sample Y to X | | |D_fake_loss | | |D_real_loss | | |D_X_loss | | |D_Y_loss | | |G_loss | | </p> <h4 id="cat_10deluxe_instance_patch_cycle_naive">cat_10deluxe_instance_patch_cycle_naive</h4> <p>| Title | Image | | :——: | :——: | |sample X to Y | | |sample Y to X | | |D_fake_loss | | |D_real_loss | | |D_X_loss | | |D_Y_loss | | |G_loss | | </p> <h4 id="cat_10deluxe_instance_patch_cycle_naive_cycle">cat_10deluxe_instance_patch_cycle_naive_cycle</h4> <p>| Title | Image | | :——: | :——: | |sample X to Y | | |sample Y to X | | |D_fake_loss | | |D_real_loss | | |D_X_loss | | |D_Y_loss | | |G_loss | | </p> <h4 id="cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug">cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug</h4> <table> <thead> <tr> <th style="text-align: center">Title</th> <th style="text-align: center">Image</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">sample X to Y</td> <td style="text-align: center"> </td> </tr> <tr> <td style="text-align: center">sample Y to X</td> <td style="text-align: center"> </td> </tr> <tr> <td style="text-align: center">D_fake_loss</td> <td style="text-align: center"> </td> </tr> <tr> <td style="text-align: center">D_real_loss</td> <td style="text-align: center"> </td> </tr> <tr> <td style="text-align: center">D_X_loss</td> <td style="text-align: center"> </td> </tr> <tr> <td style="text-align: center">D_Y_loss</td> <td style="text-align: center"> </td> </tr> <tr> <td style="text-align: center">G_loss</td> <td style="text-align: center"> </td> </tr> </tbody> </table> <h4 id="apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug">apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug</h4> <p>| Title | Image | | :——: | :——: | |sample X to Y | | |sample Y to X | | |D_fake_loss | | |D_real_loss | | |D_X_loss | | |D_Y_loss | | |G_loss | | </p> <h4 id="observations">Observations:</h4> <p>We observed that the results with the cycle-consistency loss were better than the results without it. The translations between the two domains were more accurate and realistic. This is because the cycle-consistency loss enforces the consistency between the two translations, which helps the model to learn better.</p> <p>We also observed that the DCDiscriminator resulted in better quality translations than the PatchDiscriminator. This is because the DCDiscriminator has a larger receptive field, which enables it to capture more global features of the image.</p> <h4 id="conclusion">Conclusion:</h4> <p>In conclusion, we have trained CycleGAN from scratch with and without the cycle-consistency loss, and have compared the results using the DCDiscriminator and the PatchDiscriminator. We have observed that the cycle-consistency loss and the DCDiscriminator resulted in better quality translations between the two domains. These observations can help in improving the translation quality between different domains in image processing applications.</p> <h2 id="bells--whistles">Bells &amp; Whistles</h2> <h3 id="implement-and-train-a-diffusion-model">Implement and train a diffusion model</h3> <h4 id="training-diffusion-models-with-hugging-faces-diffusers">Training Diffusion Models with Hugging Face’s Diffusers</h4> <h4 id="introduction-1">Introduction</h4> <p>In this project, we train a simple diffusion model using the Hugging Face’s Diffusers library. Diffusion models have become state-of-the-art generative models in recent times.</p> <h4 id="key-parts-of-the-code">Key Parts of the Code</h4> <h5 id="configuration">Configuration:</h5> <p>We define a ‘TrainingConfig’ class that holds all the training hyperparameters. Hyperparameters include ‘image_size’, ‘train_batch_size’, ‘eval_batch_size’, ‘num_epochs’, ‘gradient_accumulation_steps’, ‘learning_rate’, and ‘lr_warmup_steps’, among others.</p> <h5 id="data-preprocessing">Data Preprocessing:</h5> <p>We use the datasets library to load our dataset and apply data transformations. The dataset is preprocessed using the transforms.Compose function from torchvision. The dataset is then transformed on-the-fly during training.</p> <h5 id="model-definition">Model Definition:</h5> <p>We define our model using the ‘UNet2DModel’ class from the diffusers library. The model has various hyperparameters such as ‘sample_size’, ‘in_channels’, ‘out_channels’, ‘layers_per_block’, ‘block_out_channels’, ‘down_block_types’, and ‘up_block_types’. &lt;!– ```python from diffusers import UNet2DModel</p> <p>model = UNet2DModel( sample_size=config.image_size, # the target image resolution in_channels=3, # the number of input channels, 3 for RGB images out_channels=3, # the number of output channels layers_per_block=2, # how many ResNet layers to use per UNet block block_out_channels=(128, 128, 256, 256, 512, 512), # the number of output channes for each UNet block down_block_types=( “DownBlock2D”, # a regular ResNet downsampling block “DownBlock2D”, “DownBlock2D”, “DownBlock2D”, “AttnDownBlock2D”, # a ResNet downsampling block with spatial self-attention “DownBlock2D”, ), up_block_types=( “UpBlock2D”, # a regular ResNet upsampling block “AttnUpBlock2D”, # a ResNet upsampling block with spatial self-attention “UpBlock2D”, “UpBlock2D”, “UpBlock2D”, “UpBlock2D”<br> ), ) –&gt; &lt;!– ##### Noise Scheduler:</p> <p>We use the ‘DDPMScheduler’ class from the diffusers library to define the noise scheduler for our model. The scheduler takes a batch of images, a batch of random noise, and the timesteps for each image.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">diffusers</span> <span class="kn">import</span> <span class="n">DDPMScheduler</span>

<span class="n">noise_scheduler</span> <span class="o">=</span> <span class="nc">DDPMScheduler</span><span class="p">(</span><span class="n">num_train_timesteps</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="sb">``</span><span class="err">`</span> <span class="o">--&gt;</span>
<span class="c1">#### Training Setup:
</span>
<span class="n">We</span> <span class="n">use</span> <span class="n">an</span> <span class="n">AdamW</span> <span class="n">optimizer</span> <span class="ow">and</span> <span class="n">a</span> <span class="n">cosine</span> <span class="n">learning</span> <span class="n">rate</span> <span class="n">schedule</span> <span class="k">for</span> <span class="n">training</span><span class="p">.</span>
<span class="n">We</span> <span class="n">use</span> <span class="n">the</span> <span class="n">DDPMPipeline</span> <span class="k">class</span> <span class="nc">from</span> <span class="n">the</span> <span class="n">diffusers</span> <span class="n">library</span> <span class="k">for</span> <span class="n">end</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">end</span> <span class="n">inference</span> <span class="n">during</span> <span class="n">evaluation</span><span class="p">.</span>
<span class="n">The</span> <span class="n">training</span> <span class="n">function</span> <span class="n">train_loop</span> <span class="ow">is</span> <span class="n">defined</span><span class="p">,</span> <span class="n">which</span> <span class="n">includes</span> <span class="n">gradient</span> <span class="n">accumulation</span><span class="p">,</span> <span class="n">mixed</span> <span class="n">precision</span> <span class="n">training</span><span class="p">,</span> <span class="ow">and</span> <span class="n">multi</span><span class="o">-</span><span class="n">GPU</span> <span class="ow">or</span> <span class="n">TPU</span> <span class="n">training</span> <span class="n">using</span> <span class="n">the</span> <span class="n">Accelerator</span> <span class="k">class</span> <span class="nc">from</span> <span class="n">the</span> <span class="n">accelerate</span> <span class="n">library</span><span class="p">.</span>
<span class="o">&lt;</span><span class="err">!</span><span class="o">--</span> 
<span class="sb">``</span><span class="err">`</span><span class="n">python</span>
<span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
    <span class="n">clean_images</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="sh">'</span><span class="s">images</span><span class="sh">'</span><span class="p">]</span>
    <span class="c1"># Sample noise to add to the images
</span>    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">clean_images</span><span class="p">.</span><span class="n">shape</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">clean_images</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">bs</span> <span class="o">=</span> <span class="n">clean_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Sample a random timestep for each image
</span>    <span class="n">timesteps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise_scheduler</span><span class="p">.</span><span class="n">num_train_timesteps</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">clean_images</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>

    <span class="c1"># Add noise to the clean images according to the noise magnitude at each timestep
</span>    <span class="c1"># (this is the forward diffusion process)
</span>    <span class="n">noisy_images</span> <span class="o">=</span> <span class="n">noise_scheduler</span><span class="p">.</span><span class="nf">add_noise</span><span class="p">(</span><span class="n">clean_images</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>
    
    <span class="k">with</span> <span class="n">accelerator</span><span class="p">.</span><span class="nf">accumulate</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
        <span class="c1"># Predict the noise residual
</span>        <span class="n">noise_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">noisy_images</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">noise_pred</span><span class="p">,</span> <span class="n">noise</span><span class="p">)</span>
        <span class="n">accelerator</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

        <span class="n">accelerator</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="n">lr_scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            
</code></pre></div></div> <h4 id="training-execution-">Training Execution: –&gt;</h4> <p>We use the ‘notebook_launcher’ function from the accelerate library to launch the training from the notebook.</p> <h4 id="key-functions">Key Functions</h4> <p><em>transform(examples)</em>: Applies the image transformations on the fly during training. <em>evaluate(config, epoch, pipeline)</em>: Generates a batch of sample images during evaluation and saves them as a grid to the disk. <em>train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)</em>: The main training loop, which includes the forward diffusion process, loss calculation, and backpropagation.</p> <h4 id="diffusion-results">Diffusion Results</h4> <table> <thead> <tr> <th style="text-align: center">Title</th> <th style="text-align: center">Image</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Apple</td> <td style="text-align: center"> </td> </tr> <tr> <td style="text-align: center">Cat</td> <td style="text-align: center"> </td> </tr> </tbody> </table> <p>The quality of the generated images and how well the DCGAN has captured the main differences between the two domains depend on factors such as the quality of the training data, hyperparameters used during training, and complexity of image domains. If the diffusion results look unrealistic compared to the DCGAN results, it could be due to factors such as dataset quality, model complexity, hyperparameter tuning, or training time. Further analysis and experimentation would be necessary to pinpoint the specific reason for the difference in image quality. </p> <p>–&gt;</p> <h2 id="conclusion-1">Conclusion</h2> <p>This report presents our implementation of DCGAN and CycleGAN for various image generation tasks. Through these experiments, we have observed the impact of data augmentation and differentiable augmentation on the training process and final results. We have also seen the capabilities of CycleGAN in generating realistic images for domain-to-domain translation tasks, such as converting Grumpy cats to Russian Blue cats and vice versa, and converting apples to oranges and vice versa.</p> </body></html>