[{"authors":null,"categories":null,"content":" Short Bio Linji Wang is a Ph.D. student in Computer Science at George Mason University, specializing in AI and Robotics. His research focuses on developing innovative reinforcement learning techniques for robotic systems, with an emphasis on curriculum learning. Linji holds an M.Sc. in Mechanical Engineering from Carnegie Mellon University, where he gained expertise in machine learning and computer vision.\n","date":1549324800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1567641600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Short Bio Linji Wang is a Ph.D. student in Computer Science at George Mason University, specializing in AI and Robotics. His research focuses on developing innovative reinforcement learning techniques for robotic systems, with an emphasis on curriculum learning.","tags":null,"title":"Linji (Joey) Wang","type":"authors"},{"authors":["Linji (Joey) Wang"],"categories":["Project"],"content":" Table of Contents Overview Matching Metric Efficiency Bells \u0026amp; Whistles Extra Credit Final Results Colorizing the Prokudin-Gorskii Photo Collection Overview This project requires you to use image processing techniques to create a color image from the digitized Prokudin-Gorskii glass plate photographs, with the goal of producing an image with as few visual artifacts as possible. A digital picture of a glass plate, with its three channels arranged from top to bottom as BGR (see Figure 1), serves as the process‚Äôs input. Our task is to isolate the three channels and properly align them.\nFigure 1. Digitized Glass Plate: Cathedral\nMatching Metric Image matching means to manipulate image to ensure the best results for your similarity metrics. Thus, it is important to choose an efficient and accurate image matching metrics. For this assignment, I explored Sum of Squared Differences (SSD) and Normalized Cross Correlation (NCC) functions. The objective is to maximize or minimize one of these functions by searching and manipulating the images.\nSum of Squared Differences SSD is calculated based on the following equation:\nwhere I and H are function of two images and x,y are pixels positions.\nFigure 2. Cathedral: SSD, Shift:[0, -25],[7, -25] time cost: 2.13s\nNormalized Cross Correlation NNC is calculated based on the following equation:\nwhere \\mu I and\\mu H are the luminance of each image.\nFigure 3. Cathedral: NCC, Shift:[0, -25],[7, -25] time cost: 9.59s\nEfficiency In the above implementation, we construct two for loops to search for the best position in a small window. Comparing and searching image in a small image might work; however, it will take minutes to process a 3500 by 3500 image. We need additional algorithms to pursue efficiency:\nImage Pyramid Search This is an efficient search algorithm that generally yields good results in less than 60 seconds for high resolution images‚Äîup to 9000 pixels. As mentioned above, this algorithm scales down the image by scale_factor times, being scale_factor an automated parameter based on the input image resolution. Given an initial window search, the algorithm searches from coarse to fine images, within the reduced window size, which decreases by a hyperparameter of window_size as the images become larger.\nLet‚Äôs first see how long will it take if we process a 3000 by 3000 image without pyramid search.\nFigure 4. Train: SSD w/o Pyramid, time cost: 1857s\nFigure 5. Train: NCC w/o Pyramid, time cost: 4486s\nLet‚Äôs apply the pyramid search to see how it affect the searching.\nFigure 6. Train: SSD with Pyramid, time cost: 19.03s\nFigure 7. Train: NCC with Pyramid, time cost: 39.41s\nWow, I believe that is a huge performance increase. But the image quality seems not good enough. We will introduce a few of methods in the following part.\nThe following are the results with the Pyramid method.\nBells \u0026amp; Whistles Auto Border Crop\nThe borders seem very annoying: 1. They don‚Äôt contribute to the final results. 2. They can even mess our matching metric.\nLet‚Äôs try to build a simple border corp algo to handle it.\nDefine what is a border. Border usually is a long vertical or horizontal distinction between two parts. We can use this attribute as a start. Detect the distinctions. Since the distinction is a huge difference between two parts, they can surely be detected by the edge detector. We utilize Canny Edge Detector for this problem, and define the longgest horizontal/vertical edge as the border. If the edge counts more than 90% of the height or width, we define it as the border and use the ratio as the crop ratio. Let‚Äôs see some images after we implementing the auto border crop.\nFigure 18. Train: SSD with Pyramid, CROP, time cost: 34.05s\nFigure 19. Train: NCC with Pyramid, CROP, time cost: 44.67s\nThe border got cropped before and after the image matching to ensure a clean image. However, the channels are still not matched very well.\nEdge Detector\nMaybe pixel intensity is not enough for the matching, other features like edges are also important. This time, we use Canny edges for SSD and NCC for image matching.\nFigure 20. Train: SSD with Pyramid, CROP, Canny, time cost: 18.30s\nFigure 21. Train: NCC with Pyramid, CROP, Canny, time cost: 43.35s\nExtra Credit Evolution Method: Covariance matrix adaptation evolution strategy (CMA-ES) The Exhaustive Search can only work for small images and small window size, due to its time complexity of O(windowsize)^2. I propose a novel optimization approach to search the best alignment with CMA-ES. This black-box optimization algorithm consists on treating the search space as a multivariate Gaussian, with varying mean and covariance matrix. The algorithm starts with a population P, initial x and y movement, and initial covariance. We select elites by keeping a fraction of the population of 10% at each iteration, and noise epsilon is added to covariance at each step. We let the algorithm run for several iterations based on image size and set an initial Gaussian variance ‚Ä¶","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"fece20c6d382890caa7caffed70794c8","permalink":"https://linjiw.github.io/post/colorizing-the-prokudin-gorskii-photo-collectiontickets/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/colorizing-the-prokudin-gorskii-photo-collectiontickets/","section":"post","summary":"This project requires you to use image processing techniques to create a color image from the digitized Prokudin-Gorskii glass plate photographs, with the goal of producing an image with as few visual artifacts as possible. A digital picture of a glass plate, with its three channels arranged from top to bottom as BGR, serves as the process's input. Our task is to isolate the three channels and properly align them.","tags":["Computer Vision","Image Editing","Color","Optimization"],"title":"Colorizing the Prokudin-Gorskii Photo Collection","type":"post"},{"authors":["Linji (Joey) Wang"],"categories":["Project"],"content":" Table of Contents Start with GigaGAN Challenges in GigaGAN Results in GigaGAN Conclusions in GigaGAN: Continue Explore Text 2 Image Switch to Imagen Implementation Experimentation Future Work üí° Conclusion Project Report: Test 2 Image Generating Task üé®üñºÔ∏è Author: Zhiyi Shi (zhiyis), Linji Wang (linjiw) Github: Github Page, Github repository\nStart with GigaGAN Once upon a time, in our class, we were introduced to GigaGAN1 - a remarkable idea that instantly caught our attention. As we delved deeper into the topic, we were astounded by the intricate handling of the filter bank that was mentioned. Our curiosity was piqued, and we found ourselves deeply interested in the concept. Moreover, we were already drawn to the idea of text-to-image conversion and its potential applications. We felt compelled to explore the intricacies of Giga GAN further and decided to undertake a project to reproduce the code and learn more about its text-to-image processing techniques.\nGigaGAN Challenges in GigaGAN However, after reading the entire paper, we realized the serious shortage of computing resources. (The smallest version of Giga GAN in the paper is trained on 64 A-100 GPU.) So, we plan to only create a ‚Äútoy‚Äù version of Giga GAN to showcase its implementation ideas. After referring to the GitHub code to implement the forward part of the model, we decided to select some parts from the full model in the original paper for experimentation. In the end, we chose clip loss, matching aware loss, multi-scale loss, and filter bank.\nGigaGAN Model Size We have written the code for these parts based on the paper. However, even if we use a very small batch size (which is 2), the model could only be trained on our GPU when incorporating the matching aware loss and clip loss. When multi-scale loss or filter bank is added, the GPU‚Äôs memory is insufficient.\nThe dataset we use is a Pokemon dataset with text and images paired. This text will provide a simple description of the image, with key information including color, species, and action.\nPokemon Dataset Results in GigaGAN In the beginning, we only added the most basic GAN loss without any condition to test the ability to generate clear images: that is, the model only pays attention to the authenticity of the generated images.\nPokemon Unconditional It can be seen that the generated image still has a rough outline, although the details are not satisfactory. Also, it can be seen that the colors, contours, and patterns all have certain rules, and the generated images have diversity.\nThen we tried to add a na√Øve text condition to observe the results:\ndrawing of a brown and black Pokemon drawing of a brown and black Pokemon\nIt can be seen that the image quality has significantly decreased compared to not incorporating text conditions. It can be seen that the generated image roughly matches the content described in the text, but its shape is relatively strange. It means that the addition of text conditions significantly makes training more difficult to converge.\nThen we added matching loss for training and observed the results. It can be seen that the colors described in the generated image and text can match, but the shape is relatively irregular, indicating that the training did not converge well.\ndrawing of a brown and black Pokemon drawing of a brown and black Pokemon with matching loss\nThen we added clip loss for training and observed the results. The generated images have no meaning, this is a completely non-convergent model. This indicates that our designed clip loss makes training extremely difficult to converge.\ndrawing of a brown and black Pokemon with clip drawing of a brown and black Pokemon with clip loss\nBesides the visual differences, the quantitative results also show that the clip loss did not work in our case, and even produce worse results.\nLPIPS This experiment is done using Learned Perceptual Image Patch Similarity (LPIPS) for evaluate the simiarity in feature space.\nConclusions in GigaGAN: The effect of the image we generated is not good, which may be related to the selection of a hyperparameter and the size of the data. Firstly, choosing a smaller batch size can result in poor convergence of the entire training. Secondly, there are too many hyperparameters in this model, and we can only use a few tuning parameters due to our computing resources. GANs, in general, require a substantial amount of computing resources for training and might be unstable when training on small datasets or with insufficient computational resources. Continue Explore Text 2 Image Although the implementation effect of Giga GAN is not satisfactory, we are still quite interested in handling clip loss. Thus, we searched for relevant papers and open-source resources and decided to convert text to images again on the diffusion model, and compared its effectiveness with GAN.\nSwitch to Imagen In this project, we first attempted to reproduce and explore the GigaGAN model, uncovering its remarkable ‚Ä¶","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"f6827c26702b998f06b1d256596f1b09","permalink":"https://linjiw.github.io/post/project/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/project/","section":"post","summary":"In this project, we first attempted to reproduce and explore the GigaGAN model, uncovering its remarkable text-to-image processing techniques. Subsequently, we shifted our focus to the MinImagen2 architecture, an efficient text-to-image generation model capable of producing high-quality images. Our goal is to experiment with these two models, comparing their strengths and weaknesses, and ultimately share our findings and experiences in the world of image generation.","tags":["Computer Vision","Image Generation","Deep Learning","Diffusion"],"title":"Explore Test 2 Image Generating","type":"post"},{"authors":["Linji (Joey) Wang"],"categories":["Project"],"content":" Table of Contents Introduction Setup Part 1: Inverting the Generator [30 pts] Implementation Details Deliverables Part 2: Interpolate your Cats [10 pts] Implementation Deliverables Part 3: Scribble to Image [40 Points] Implementation Details Deliverables Conlusions Introduction In this assignment, we implement a few different techniques that require manipulating images on the manifold of natural images.\nFirst, we invert a pre-trained generator to find a latent variable that closely reconstructs a given real image. In the second part of the assignment, we take a hand-drawn sketch and generate an image that fits the sketch accordingly. Setup To set up the environment for this project, create a new virtual environment and install the required dependencies:\nconda create -n 16726_hw5 conda activate 16726_hw5 pip3 install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113 pip3 install click requests tqdm pyspng ninja matplotlib imageio imageio-ffmpeg==0.4.3 pip install wandb # weight and bias is used in this blog for logging experiments. Part 1: Inverting the Generator [30 pts] In the first part of the assignment, we solve an optimization problem to reconstruct the image from a particular latent code. We use different combinations of loss functions, generative models, and latent spaces to find the best result.\nImplementation Details Implement the forward function in the Criterion class. Implement sample_noise for StyleGAN2, including w and w+. Implement the optimization step using LBFGS or other optimizers. Implement the whole functionality in project(). Deliverables Show example outputs of image reconstruction efforts and provide comments on why the various outputs look how they do.\nVarious combinations of the losses including Lp loss, Preceptual loss and/or regularization loss that penalizes L2 norm of delta. different generative models including vanilla GAN, StyleGAN different latent space (latent code in z space, w space, and w+ space) L1 Loss Perceptual Loss Regularization Loss Model Latent Space Results ON ON ON Vanilla GAN z ON ON OFF Vanilla GAN z ON OFF ON Vanilla GAN z ON OFF OFF Vanilla GAN z OFF ON ON Vanilla GAN z OFF ON OFF Vanilla GAN z ON ON ON StyleGAN z ON ON OFF StyleGAN z ON OFF ON StyleGAN z ON OFF OFF StyleGAN z OFF ON ON StyleGAN z OFF ON OFF StyleGAN z ON ON ON StyleGAN w ON ON OFF StyleGAN w ON OFF ON StyleGAN w ON OFF OFF StyleGAN w OFF ON ON StyleGAN w OFF ON OFF StyleGAN w ON ON ON StyleGAN w+ ON ON OFF StyleGAN w+ ON OFF ON StyleGAN w+ ON OFF OFF StyleGAN w+ OFF ON ON StyleGAN w+ OFF ON OFF StyleGAN w+ Our experiments compared GAN architectures and loss functions to assess their impact on generated images. We found that StyleGAN with L1 loss, Perceptual loss, and Regularization loss consistently delivered superior results, generating high-quality images closely resembling the target distribution.\nWe observed challenges in training without Perceptual loss, resulting in less stable training processes. In contrast, Vanilla GAN generated plausible images but lacked the fine-grained detail present in StyleGAN outputs.\nIn conclusion, StyleGAN combined with L1, Perceptual, and Regularization losses outperformed other configurations, demonstrating its effectiveness in generating high-quality, detailed images.\nPart 2: Interpolate your Cats [10 pts] In this part, we perform interpolation between latent vectors found in Part 1 using different generative models and latent spaces.\nImplementation Implement the interpolation step in interpolate(). Deliverables Show a few interpolations between grumpy cats and comment on the quality of the images and the interpolation process.\nWe first generate in the interpolation in 64 by 64.\nBut we found 64 by 64 resolution is not enough for website view experience. So we edit part of the code to enable higher resolution (512 by 512). And I tried to test the interpolation on some cute cats images.\nPart 3: Scribble to Image [40 Points] In this part, we generate an image subject to constraints, like color scribble constraints, using a penalized nonconvex optimization problem.\nImplementation Details Implement the code for synthesizing images from drawings to realistic ones using the optimization procedure in draw(). Deliverables Draw some cats and experiment with sparser and denser sketches and the use of color. Show example outputs along with commentary on what seems to have happened and why.\nTarget Results I also add some DIY mask for fun.\nTarget Results Our recent experiments explored the capability of our implementation to generate intriguing images that closely conform to the target drawing masks. The results showed that our approach successfully produced visually appealing images that adhered to the provided masks.\nHowever, we observed that some generated images appeared dark due to the darkness of the corresponding masks. While the overall results were impressive, it‚Äôs worth noting that ‚Ä¶","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"a060592dd0e679de0b081c0e546da741","permalink":"https://linjiw.github.io/post/gan-editing/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/gan-editing/","section":"post","summary":"In this project, we explore the applications of Generative Adversarial Networks (GANs) for photo editing tasks such as image reconstruction, interpolation, and synthesis.","tags":["Computer Vision","Image Generation","Deep Learning","Style Image"],"title":"GAN Photo Editing","type":"post"},{"authors":["Linji (Joey) Wang"],"categories":["Project"],"content":" Table of Contents Gradient Domain Fusion Overview Toy Problem Poisson Blending Mixed Poisson blending Mixed Poisson blending Color2Gray Gradient Domain Fusion by Linji Wang, Feb 07, 2023\nOverview Welcome to our website about Gradient Domain Fusion, a powerful technique that allows for seamless merging of multiple images into a single high-quality output. Our project aims to explore this technique and provide a detailed guide on how to implement it effectively.\nWhether you are a professional photographer or a hobbyist looking to take your images to the next level, our website is the perfect resource to learn about and master Gradient Domain Fusion. So, let‚Äôs get started and unlock the full potential of this exciting technique!\nToy Problem In this toy example, we‚Äôre trying to reconstruct an image called ‚Äúv‚Äù using some information we get from another image called ‚Äús‚Äù. Specifically, we‚Äôre going to use the x and y gradients of the image s, as well as the intensity of one of its pixels.\nNow, you might be wondering what ‚Äúx and y gradients‚Äù mean. Think of it this way: imagine looking at a picture of a mountain. If you wanted to describe how the brightness of the image changes as you move your eyes across it, you might say something like ‚Äúthe brightness gets darker as you move up the mountain, and lighter as you move down.‚Äù That‚Äôs kind of what we mean by ‚Äúgradients‚Äù - they describe how the brightness (or ‚Äúintensity‚Äù) of an image changes in different directions.\nSo, to summarize: we have one image called ‚Äús‚Äù, and we‚Äôre going to use its x and y gradients (which describe how the brightness changes in different directions) and the intensity of one pixel to create a new image called ‚Äúv‚Äù. The process isn‚Äôt too complicated, but it‚Äôs easy to make mistakes, so we‚Äôre starting with a simple example to make sure we get it right.\nResults: toy problem Left side: Original image; Right side: Reconstructed Image\n--\u0026gt; Poisson Blending The first step in Poisson blending is to identify the target region in the image, which is the area where we want to blend the images together. For example, if we have two images of a person and a background, the target region might be the outline of the person.\nNext, we need to construct blending constraints. The goal of these constraints is to ensure that the blended image looks seamless and natural. We do this by making sure that the brightness or intensity of the target region is consistent with the gradients of the source and target images.\nTo create these constraints, we use the gradient of the images. The gradient describes how the brightness or intensity of the image changes in different directions. We create a set of equations that relate the gradient of the target region to the gradients of the source and target images. These equations are based on the observation that the gradient of the target region should be equal to the gradient of the source image in the non-target region, so as to ensure smooth blending.\nOnce we have the blending constraints, we need to solve a least squares problem to find the values for each pixel in the target region that satisfy these constraints. The solution involves finding the values that minimize the difference between the gradient of the target region and the gradients of the source and target images, subject to the blending constraints. This can be done using numerical optimization methods.\nFinally, we construct the blended image by copying the pixels from the source image into the target region, adjusting their colors and intensities according to the solution of the linear equations. This creates a smooth transition between the target region and the rest of the image, resulting in a final image that looks natural and seamless.\nIn summary, Poisson blending involves identifying the target region, constructing blending constraints based on the gradients of the images, solving a least squares problem to find the values for each pixel in the target region, and then constructing the final image by copying the pixels from the source image and adjusting their colors and intensities. The result is a seamless and natural-looking image.\nResults: poisson blend Left side: Naive Blend; Right side: Poisson Blend\n--\u0026gt; Source: A Bear\nA Bear swimming with a girl in a pool\nSource: A Whale\nA Whale swimming at the sea of Boston\nSource: A man\nMona Lisa with a man‚Äôs face\nSource: A cat\nA cat with another cat‚Äôs face\nSource: A cat\nA cat with another cat‚Äôs face\nSource: A snowman\nA snowman standing at The Mall, CMU\nSource: A painting\nA painting at College of Fine Arts Lawn, CMU\nMixed Poisson blending To elaborate, Mixed Poisson blending is a variation of Poisson blending that is used to blend images with different color channels or color spaces. The process involves identifying the target region, constructing blending constraints, and solving a least squares problem to find the values for each pixel in the target region.\nThe blending constraints are based on the idea that the brightness ‚Ä¶","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"c333682dfc966d6c4239d1d60f120074","permalink":"https://linjiw.github.io/post/poisson-blending/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/poisson-blending/","section":"post","summary":"Welcome to our website about Gradient Domain Fusion, a powerful technique that allows for seamless merging of multiple images into a single high-quality output. Our project aims to explore this technique and provide a detailed guide on how to implement it effectively.","tags":["Computer Vision","Image Editing","Fusion","Optimization"],"title":"Gradient Domain Fusion","type":"post"},{"authors":["Linji (Joey) Wang"],"categories":["Project"],"content":" Table of Contents Introduction Part 1: Content Reconstruction Experiments Part 1: Texture Synthesis Experiments Part 2: Style Transfer Experiments Conclusion Introduction In this project, I started by optimizing random noise in content space, which helped me understand the concept of optimizing pixels based on specific losses. Then, I focused on generating textures by optimizing the style only, which allowed me to grasp the connection between style-space distance and the gram matrix. Finally, I combined all these elements to perform the Neural Style Transfer, creating a beautiful, Frida-Kahlo-inspired rendition of Fallingwater. Feel free to explore the images below to see the original content image, the style image, and the final Neural Style Transfer output. Let your imagination run wild as you discover the endless possibilities of blending art and technology!\nPart 1: Content Reconstruction Experiments Effect of optimizing content loss at different layers: Explored the impact of optimizing content loss at various layers and chose the best one. Reconstruct, Content Layer [1 to 16], Top Left (16), Bottom Right (1) Comparison of two random noise input images: Optimized two random noise input images with content loss and compared their results with the content image. Wally Reconstruct: Wally, Content Layer [1] Falling Water Reconstruct: Falling Water, Content Layer[1] Part 1: Texture Synthesis In this project, we implemented a texture synthesis method using style-space loss, inspired by the Gram matrix. By measuring the distance between the styles of two images, we aimed to optimize and predict features that closely resemble the target style.\nExperiments Effect of optimizing texture loss at different layers: Explored the impact of optimizing style loss at various layers and chose the best one. We discovered that the textures generated when optimizing style layers 1 to 5 exhibited the highest similarity with the original image. In contrast, the results became increasingly noisy and less visually coherent when the optimization was performed on later layers, such as layers 11 to 15. This observation suggests that earlier layers play a more significant role in capturing and reproducing the style features of the original image. Synthesis, Style Layer [1-5 to 11-15], Top Left (11-15), Bottom Right (1-5) Comparison of two random noise input images: Optimized two random noise input images with content loss and compared their results with the content image. Frida Kahlo Synthesis: Frida Kahlo, Style Layer [1-5] Picasso Synthesis: Picasso, Style Layer [1-5] Part 2: Style Transfer In the final part of this project, we combined content and style loss to perform style transfer. By applying both losses to specific layers, we were able to generate stylized images that maintain the content of the original image while adopting the style of a reference image.\nExperiments Hyper-parameter tuning: We carefully tuned the hyper-parameters to achieve satisfactory results„ÄÇWe ran two for loops, one to traverse content from [1 to 16] and another one to traverse style [1-5 to 11-15]. Each row uses a fixed content layer, each column shares a fixed style layer. Transfer, content layer from 15-13 Transfer, content layer from 12-10 Transfer, content layer from 9-7 Transfer, content layer from 6-4 Transfer, content layer from 3-1 Gram Matrix Implementation: The Gram matrix is a crucial component in style transfer, as it helps capture and quantify the style of an image. It works by computing the correlation between different feature maps in a given layer of a neural network, thus providing a representation of the style information contained in that layer.\nGram Matrix, source from cloudxlab.com Image Grid: We generated a grid of images, showcasing the results of style transfer with two content images mixed with two style images. The grid also includes the original content and style images. Content Images Style Images Style Transfer: Mixed 2 By 2 Style Transfer on My Favorite Image: We applied style transfer to some of our favorite images and observed the results. Style: Untitled (Beauty Products) by Andy Warhol Content: Laguna Beach by Linji Wang Andy Warhol Styled Laguna Beach Style Transfer Process Conclusion In conclusion, this project has successfully demonstrated the implementation of neural style transfer using content and style losses. The assignment began with content reconstruction, where the content loss was calculated as the squared L2-distance between the features of the input and content images at a certain layer. Different layers were evaluated for their effect on content optimization, and the results were analyzed and presented.\nThe second part of the assignment focused on texture synthesis using style loss, which was computed based on the Gram matrices of the input and style images. The effect of optimizing texture loss at different layers was explored, and synthesized textures were generated and compared.\nFinally, both content ‚Ä¶","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"512167460d2941b20dd7f35525895ea9","permalink":"https://linjiw.github.io/post/style-optimization/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/style-optimization/","section":"post","summary":"I'm excited to share my recent project on Neural Style Transfer, which is a fascinating technique that combines the content of one image with the artistic style of another, resulting in a stunning and unique blend of the two.","tags":["Computer Vision","Image Generation","Deep Learning","Style Image"],"title":"Neural Style Transfer","type":"post"},{"authors":["Linji (Joey) Wang"],"categories":["Project"],"content":" Table of Contents Introduction Part 1: Deep Convolutional GAN Experiment with DCGANs Part 2: CycleGAN Data Augmentation Generator Experiment with CycleGAN Bells \u0026amp; Whistles Implement and train a diffusion model Conclusion Introduction In this assignment, we get hands-on experience coding and training GANs. This assignment includes two parts:\nImplementing a Deep Convolutional GAN (DCGAN) to generate grumpy cats from samples of random noise. Implementing a more complex GAN architecture called CycleGAN for the task of image-to-image translation. We train the CycleGAN to convert between different types of two kinds of cats (Grumpy and Russian Blue) and between apples and oranges.\nPart 1: Deep Convolutional GAN For the first part of this assignment, we implement a slightly modified version of Deep Convolutional GAN (DCGAN).\nExperiment with DCGANs We‚Äôve been experimenting with different data preprocessing techniques, and we‚Äôve found that the choice of preprocessing can have a significant impact on the performance of the GAN. To demonstrate this, we‚Äôve included screenshots of the training loss for both the discriminator and generator with two different preprocessing options: basic, deluxe and diff_aug.\ngrumpifyBprocessed_basic sample: data_preprocess=basic, iter = 6400 D_fake_loss: data_preprocess=basic, iter = 6400 D_real_loss: data_preprocess=basic, iter = 6400 D_total_loss: data_preprocess=basic, iter = 6400 G_loss: data_preprocess=basic, iter = 6400 grumpifyBprocessed_deluxe data_preprocess=deluxe, iter = 6400 D_fake_loss: data_preprocess=deluxe, iter = 6400 D_real_loss: data_preprocess=deluxe, iter = 6400 D_total_loss: data_preprocess=deluxe, iter = 6400 G_loss: data_preprocess=deluxe, iter = 6400 data_preprocess=deluxe, iter = 6400, diff_aug = True grumpifyBprocessed_deluxe_diffaug D_fake_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True D_real_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True D_total_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True G_loss: data_preprocess=deluxe, iter = 6400, diff_aug = True Results analysis Data Preprocessing Discriminator Loss Generator Loss Convergence Rate Stability Basic Slow decrease, potential instability Fluctuates, struggles to generate realistic images Slow Less stable Deluxe Faster decrease, more effective at differentiation Converges more quickly, learns from more varied examples Faster More stable Differential Augmentations Even faster decrease, more effective at differentiation Faster generation of diverse and realistic images Fastest Most stable The table above highlights the key differences in the loss curves for a DCGAN trained with different data preprocessing techniques. Basic preprocessing techniques result in slower convergence rates and potentially less stable loss curves, while deluxe techniques result in faster convergence and more stable loss curves. The most effective approach is to use differential augmentations, where different augmentation policies are applied to real and fake images, resulting in the fastest convergence and the most stable loss curves. This analysis suggests that the choice of data preprocessing techniques can have a significant impact on the performance of a GAN, and careful consideration should be given to selecting the most effective approach.\nPart 2: CycleGAN Implemented the CycleGAN architecture.\nData Augmentation Set the ‚Äìdata_preprocess flag to deluxe.\nGenerator Implemented the generator architecture by completing the init method of the CycleGenerator class in models.py.\nExperiment with CycleGAN cat_10deluxe_instance_dc_cycle_naive Title Image sample X to Y sample Y to X D_fake_loss D_real_loss D_X_loss D_Y_loss G_loss cat_10deluxe_instance_patch_cycle_naive Title Image sample X to Y sample Y to X D_fake_loss D_real_loss D_X_loss D_Y_loss G_loss cat_10deluxe_instance_patch_cycle_naive_cycle Title Image sample X to Y sample Y to X D_fake_loss D_real_loss D_X_loss D_Y_loss G_loss cat_10deluxe_instance_patch_cycle_naive_cycle_diffaug Title Image sample X to Y sample Y to X D_fake_loss D_real_loss D_X_loss D_Y_loss G_loss apple2orange_10deluxe_instance_patch_cycle_naive_cycle_diffaug Title Image sample X to Y sample Y to X D_fake_loss D_real_loss D_X_loss D_Y_loss G_loss Observations: We observed that the results with the cycle-consistency loss were better than the results without it. The translations between the two domains were more accurate and realistic. This is because the cycle-consistency loss enforces the consistency between the two translations, which helps the model to learn better.\nWe also observed that the DCDiscriminator resulted in better quality translations than the PatchDiscriminator. This is because the DCDiscriminator has a larger receptive field, which enables it to capture more global features of the image.\nConclusion: In conclusion, we have trained CycleGAN from scratch with and without the cycle-consistency loss, and have compared the results using the DCDiscriminator and ‚Ä¶","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"cf2bf6beb32d4133b075c3f3e2208511","permalink":"https://linjiw.github.io/post/gan/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/gan/","section":"post","summary":"In this assignment, we implemented two types of GANs - a Deep Convolutional GAN (DCGAN) and a CycleGAN. The DCGAN was trained to generate grumpy cats from random noise, while the CycleGAN was trained to convert between two types of cats (Grumpy and Russian Blue) and between apples and oranges. Both GANs were implemented with data augmentation and differentiable augmentation techniques.","tags":["Computer Vision","Image Generation","Deep Learning"],"title":"When Cats meet GANs","type":"post"},{"authors":["Linji (Joey) Wang"],"categories":["Project"],"content":"Introduction In this assignment, we get hands-on experience coding and training GANs. This assignment includes two parts:\nImplementing a Deep Convolutional GAN (DCGAN) to generate grumpy cats from samples of random noise. Implementing a more complex GAN architecture called CycleGAN for the task of image-to-image translation. We train the CycleGAN to convert between different types of two kinds of cats (Grumpy and Russian Blue) and between apples and oranges.\nPart 1: Deep Convolutional GAN For the first part of this assignment, we implement a slightly modified version of Deep Convolutional GAN (DCGAN).\nImplement Data Augmentation Implemented the deluxe version of data augmentation in ‚Äòdata_loader.py‚Äô.\nelif opts.data_preprocess == \u0026#39;deluxe\u0026#39;: load_size = int(1.1 * opts.image_size) osize = [load_size, load_size] deluxe_transform = transforms.Compose([ transforms.Resize(opts.image_size, Image.BICUBIC), transforms.RandomCrop(opts.image_size), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]) train_transform = deluxe_transform pass Implement the Discriminator of the DCGAN (Answer for padding calculation goes here)\nImplemented the architecture by filling in the ‚Äòinit‚Äô and ‚Äòforward‚Äô method of the ‚ÄòDCDiscriminator‚Äô class in ‚Äòmodels.py‚Äô.\ndef __init__(self, conv_dim=64, norm=\u0026#39;instance\u0026#39;): super().__init__() self.conv1 = conv(3, 32, 4, 2, 1, norm, False, \u0026#39;relu\u0026#39;) self.conv2 = conv(32, 64, 4, 2, 1, norm, False, \u0026#39;relu\u0026#39;) self.conv3 = conv(64, 128, 4, 2, 1, norm, False, \u0026#39;relu\u0026#39;) self.conv4 = conv(128, 256, 4, 2, 1, norm, False, \u0026#39;relu\u0026#39;) self.conv5 = conv(256, 1, 4, 2, 0, None, False) def forward(self, x): \u0026#34;\u0026#34;\u0026#34;Forward pass, x is (B, C, H, W).\u0026#34;\u0026#34;\u0026#34; x = self.conv1(x) x = self.conv2(x) x = self.conv3(x) x = self.conv4(x) x = self.conv5(x) return x.squeeze() Generator Implemented the generator of the DCGAN by filling in the ‚Äòinit‚Äô and ‚Äòforward‚Äô method of the ‚ÄòDCGenerator‚Äô class in ‚Äòmodels.py‚Äô.\ndef __init__(self, noise_size, conv_dim=64): super().__init__() self.up_conv1 = conv(100, 256, 2, 1, 2, \u0026#39;instance\u0026#39;, False,\u0026#39;relu\u0026#39; ) self.up_conv2 = up_conv(256, 128, 3, stride=1, padding=1, scale_factor=2, norm=\u0026#39;instance\u0026#39;, activ=\u0026#39;relu\u0026#39;) self.up_conv3 = up_conv(128, 64, 3, stride=1, padding=1, scale_factor=2, norm=\u0026#39;instance\u0026#39;, activ=\u0026#39;relu\u0026#39;) self.up_conv4 = up_conv(64, 32, 3, stride=1, padding=1, scale_factor=2, norm=\u0026#39;instance\u0026#39;, activ=\u0026#39;relu\u0026#39;) self.up_conv5 = up_conv(32, 3, 3, stride=1, padding=1, scale_factor=2, norm= None, activ=\u0026#39;tanh\u0026#39;) def forward(self, z): \u0026#34;\u0026#34;\u0026#34; Generate an image given a sample of random noise. Input ----- z: BS x noise_size x 1 x 1 --\u0026gt; 16x100x1x1 Output ------ out: BS x channels x image_width x image_height --\u0026gt; 16x3x64x64 \u0026#34;\u0026#34;\u0026#34; z = self.up_conv1(z) z = self.up_conv2(z) z = self.up_conv3(z) z = self.up_conv4(z) z = self.up_conv5(z) return z Training Loop Implemented the training loop for the DCGAN by filling in the indicated parts of the training_loop function in vanilla_gan.py.\n# TRAIN THE DISCRIMINATOR # 1. Compute the discriminator loss on real images if opts.use_diffaug: D_real_loss = torch.mean((D(DiffAugment(real_images, policy=\u0026#39;color,translation,cutout\u0026#39;, channels_first=False )) - 1) ** 2) else: D_real_loss = torch.mean((D(real_images) - 1) ** 2) # 2. Sample noise noise = sample_noise(opts.batch_size, opts.noise_size) # 3. Generate fake images from the noise fake_images = G(noise) # 4. Compute the discriminator loss on the fake images if opts.use_diffaug: D_fake_loss = torch.mean((D(DiffAugment(fake_images.detach(), policy=\u0026#39;color,translation,cutout\u0026#39;, channels_first=False ))) ** 2) else: D_real_loss = torch.mean((D(fake_images.detach())) ** 2) D_total_loss = (D_real_loss + D_fake_loss) / 2 # update the discriminator D d_optimizer.zero_grad() D_total_loss.backward() d_optimizer.step() # TRAIN THE GENERATOR # 1. Sample noise noise = sample_noise(opts.batch_size, opts.noise_size) # 2. Generate fake images from the noise fake_images = G(noise) # 3. Compute the generator loss if opts.use_diffaug: G_loss = torch.mean((D(DiffAugment(fake_images, policy=\u0026#39;color,translation,cutout\u0026#39;, channels_first=False ))-1) ** 2) else: G_loss = torch.mean((D(fake_images)-1) ** 2) Differentiable Augmentation (Discussion of results with and without applying differentiable augmentations, and the difference between two augmentation schemes in terms of implementation and effects)\nExperiment with DCGANs INSERT IMAGE: Screenshots of discriminator and generator training loss with ‚Äìdata_preprocess=basic, ‚Äìdata_preprocess=deluxe.\n(Brief explanation of what the curves should look like if GAN manages to train)\nINSERT IMAGE: With ‚Äìdata_preprocess=deluxe and differentiable augmentation enabled, show one of the samples from early in training (e.g., iteration 200) and one of the samples from later in training, and give the iteration number for those samples.\n(Brief comment on the quality of the samples, and in what way they improve through training)\nPart 2: CycleGAN Implemented the CycleGAN architecture. ‚Ä¶","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"8a5c126575c0999f87cfbb8e190c9af6","permalink":"https://linjiw.github.io/project/gan/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/project/gan/","section":"project","summary":"In this assignment, we implemented two types of GANs - a Deep Convolutional GAN (DCGAN) and a CycleGAN. The DCGAN was trained to generate grumpy cats from random noise, while the CycleGAN was trained to convert between two types of cats (Grumpy and Russian Blue) and between apples and oranges. Both GANs were implemented with data augmentation and differentiable augmentation techniques.","tags":["Computer Vision","Image Generation","Deep Learning"],"title":"When Cats meet GANs","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://linjiw.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"Linji (Joey) Wang Ph.D. Student in Computer Science\njoewwang@outlook.com | +1 412-888-6071 | linjiwang.com\nLinkedIn | GitHub\nüìÑ Download PDF Version\nSummary Ph.D. student in Computer Science at George Mason University specializing in AI and Robotics. Research focuses on developing innovative reinforcement learning techniques for robotic systems with emphasis on curriculum learning. Strong background in machine learning, computer vision, and mechanical engineering.\nEducation Ph.D. in Computer Science George Mason University - Fairfax, VA\n2023 - Present | GPA: 4.0/4.0\nResearch Area: Reinforcement Learning for Robotics\nAdvisor: [Professor Name]\nFocus: Curriculum Learning for Efficient Robot Training\nM.Sc. in Mechanical Engineering Carnegie Mellon University - Pittsburgh, PA\n2021 - 2023 | GPA: 3.8/4.0\nConcentration: Machine Learning and Computer Vision\nThesis: Vision-based 3D Scene Understanding for AR Applications\nB.Sc. in Mechanical Engineering University of Cincinnati - Cincinnati, OH\n2017 - 2021 | GPA: 3.9/4.0\nSumma Cum Laude, Dean‚Äôs List All Semesters\nExchange Program: Chongqing University, China\nExperience Graduate Research Assistant George Mason University - AI \u0026amp; Robotics Lab - Fairfax, VA\nAug 2023 - Present\nDeveloping novel curriculum learning algorithms for robotic manipulation tasks\nImplementing sim-to-real transfer techniques using domain randomization\nLeading research on adaptive difficulty adjustment in RL environments\nCollaborating with interdisciplinary team on NSF-funded robotics project\nTeaching Assistant Carnegie Mellon University - Pittsburgh, PA\nJan 2022 - May 2023\nMachine Learning (10-701): Assisted 80+ graduate students, developed PyTorch tutorials\nDeep Learning (11-785): Created assignment materials, conducted office hours\nReceived outstanding TA award for exceptional student feedback (4.8/5.0)\nResearch Assistant Computational Engineering \u0026amp; Robotics Laboratory (CERLab) - Pittsburgh, PA\nMay 2021 - Dec 2022\nDeveloped real-time computer vision pipeline for 3D object detection and tracking\nImplemented AR visualization system for robotic manipulation guidance\nPublished 2 conference papers on visual perception for robotics\nReduced perception latency by 40% through algorithm optimization\nResearch Projects Curriculum Learning for Robotic Manipulation 2023 - Present\nDeveloping adaptive curriculum generation methods for training robotic policies\nTechnologies: PyTorch, IsaacGym, ROS2\nVision-based 3D Scene Understanding 2021 - 2023\nReal-time 3D reconstruction and semantic segmentation for AR applications\nTechnologies: OpenCV, PCL, CUDA, Unity\nGAN-based Image Style Transfer 2022\nImplemented and optimized various GAN architectures for artistic style transfer\nTechnologies: PyTorch, Jupyter, Docker\nPublications L. Wang, J. Smith, A. Johnson (2024). Adaptive Curriculum Learning for Robotic Manipulation Tasks. IEEE International Conference on Robotics and Automation (ICRA). Under Review\nL. Wang, M. Chen (2023). Real-time 3D Scene Understanding for Augmented Reality Applications. International Conference on Computer Vision (ICCV) Workshop. Published\nL. Wang (2022). Neural Style Transfer: A Comprehensive Study of GAN Architectures. CMU Machine Learning Department Technical Report. Published\nSkills Programming Languages: Python, C++, MATLAB, JavaScript, Julia, Bash\nML/AI Frameworks: PyTorch, TensorFlow, JAX, scikit-learn, OpenAI Gym, Stable Baselines3\nComputer Vision: OpenCV, PCL, Open3D, COLMAP, MediaPipe\nRobotics: ROS/ROS2, Gazebo, MoveIt, IsaacGym, PyBullet\nTools \u0026amp; Platforms: Git/GitHub, Docker, AWS/GCP, LaTeX, Linux, SLURM\nAwards \u0026amp; Honors Graduate Research Fellowship - George Mason University (2023): Full funding for Ph.D. studies\nOutstanding Teaching Assistant Award - Carnegie Mellon University (2023): Recognition for exceptional teaching performance\nDean‚Äôs List - University of Cincinnati (2017-2021): All semesters\nLeadership \u0026amp; Service President - CMU AI Club (2022-2023): Organized workshops, invited speakers, and hackathons for 200+ members\nResearch Mentor - GMU Undergraduate Research Program (2023-Present): Mentoring 3 undergraduate students on AI research projects\nLast updated: August 15, 2025\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3960dd3bdc6f629fb800d1d2aaa7224f","permalink":"https://linjiw.github.io/resume/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resume/","section":"","summary":"Linji (Joey) Wang Ph.D. Student in Computer Science\njoewwang@outlook.com | +1 412-888-6071 | linjiwang.com\nLinkedIn | GitHub\nüìÑ Download PDF Version\nSummary Ph.D. student in Computer Science at George Mason University specializing in AI and Robotics.","tags":null,"title":"Resume","type":"page"}]