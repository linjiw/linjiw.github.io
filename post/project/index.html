<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: August 15, 2025 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.0f229d4b7ebad1917a9a357cba2effab.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Linji (Joey) Wang"><meta name=description content="In this project, we first attempted to reproduce and explore the GigaGAN model, uncovering its remarkable text-to-image processing techniques. Subsequently, we shifted our focus to the MinImagen2 architecture, an efficient text-to-image generation model capable of producing high-quality images. Our goal is to experiment with these two models, comparing their strengths and weaknesses, and ultimately share our findings and experiences in the world of image generation."><link rel=alternate hreflang=en-us href=https://linjiw.github.io/post/project/><link rel=canonical href=https://linjiw.github.io/post/project/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hub02f11719440bf94480d1ab9e24c040b_151872_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hub02f11719440bf94480d1ab9e24c040b_151872_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:image" content="https://linjiw.github.io/post/project/featured.png"><meta property="og:site_name" content="Linji Wang"><meta property="og:url" content="https://linjiw.github.io/post/project/"><meta property="og:title" content="Explore Test 2 Image Generating | Linji Wang"><meta property="og:description" content="In this project, we first attempted to reproduce and explore the GigaGAN model, uncovering its remarkable text-to-image processing techniques. Subsequently, we shifted our focus to the MinImagen2 architecture, an efficient text-to-image generation model capable of producing high-quality images. Our goal is to experiment with these two models, comparing their strengths and weaknesses, and ultimately share our findings and experiences in the world of image generation."><meta property="og:image" content="https://linjiw.github.io/post/project/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2019-02-05T00:00:00+00:00"><meta property="article:modified_time" content="2019-09-05T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://linjiw.github.io/post/project/"},"headline":"Explore Test 2 Image Generating","image":["https://linjiw.github.io/post/project/featured.png"],"datePublished":"2019-02-05T00:00:00Z","dateModified":"2019-09-05T00:00:00Z","author":{"@type":"Person","name":"Linji (Joey) Wang"},"publisher":{"@type":"Organization","name":"Linji Wang","logo":{"@type":"ImageObject","url":"https://linjiw.github.io/media/icon_hub02f11719440bf94480d1ab9e24c040b_151872_192x192_fill_lanczos_center_3.png"}},"description":"In this project, we first attempted to reproduce and explore the GigaGAN model, uncovering its remarkable text-to-image processing techniques. Subsequently, we shifted our focus to the MinImagen2 architecture, an efficient text-to-image generation model capable of producing high-quality images. Our goal is to experiment with these two models, comparing their strengths and weaknesses, and ultimately share our findings and experiences in the world of image generation."}</script><title>Explore Test 2 Image Generating | Linji Wang</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=f6827c26702b998f06b1d256596f1b09><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Linji Wang</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Linji Wang</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/ai/><span>AI</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/robotics/><span>Robotics</span></a></li><li class=nav-item><a class=nav-link href=/sde/><span>Software</span></a></li><li class=nav-item><a class=nav-link href=/rl/><span>RL</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Explore Test 2 Image Generating</h1><p class=page-subtitle>Project Report - Test 2 Image Generating Task üé®üñºÔ∏è</p><div class=article-metadata><div><span class=author-highlighted>Linji (Joey) Wang</span></div><span class=article-date>Last updated on
Sep 5, 2019</span>
<span class=middot-divider></span>
<span class=article-reading-time>7 min read</span>
<span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/project/>Project</a></span></div></div><div class="article-header container featured-image-wrapper mt-4 mb-4" style=max-width:796px;max-height:796px><div style=position:relative><img src=/post/project/featured_huf54f067b2f6bf758cd15f06a7c71860d_1074782_1200x2500_fit_q100_h2_lanczos_3.webp width=796 height=796 alt class=featured-image>
<span class=article-header-caption>Imagen Results</span></div></div><div class=article-container><div class=article-style><!-- raw HTML omitted --><details class="toc-inpage d-print-none" open><summary class=font-weight-bold>Table of Contents</summary><nav id=TableOfContents><ul><li><a href=#start-with-gigagan>Start with GigaGAN</a><ul><li><a href=#challenges-in-gigagan>Challenges in GigaGAN</a></li><li><a href=#results-in-gigagan>Results in GigaGAN</a></li><li><a href=#conclusions-in-gigagan>Conclusions in GigaGAN:</a></li><li><a href=#continue-explore-text-2-image>Continue Explore Text 2 Image</a></li></ul></li><li><a href=#switch-to-imagen>Switch to Imagen</a><ul><li><a href=#implementation>Implementation</a></li><li><a href=#experimentation>Experimentation</a></li><li><a href=#future-work->Future Work üí°</a></li><li><a href=#conclusion>Conclusion</a></li></ul></li></ul></nav></details><h1 id=project-report-test-2-image-generating-task->Project Report: Test 2 Image Generating Task üé®üñºÔ∏è</h1><p>Author: Zhiyi Shi (<a href=zhiyis@andrew.cmu.edu>zhiyis</a>), Linji Wang (<a href=linjiw@andrew.cmu.edu>linjiw</a>)
Github: <a href=https://linjiw.github.io/Explore_Text2Image/ target=_blank rel=noopener>Github Page</a>, <a href=https://github.com/linjiw/MinImagen target=_blank rel=noopener>Github repository</a></p><h2 id=start-with-gigagan>Start with GigaGAN</h2><!-- raw HTML omitted --><p>Once upon a time, in our class, we were introduced to <strong>GigaGAN</strong><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> - a remarkable idea that instantly caught our attention. As we delved deeper into the topic, we were astounded by the intricate handling of the filter bank that was mentioned. Our curiosity was piqued, and we found ourselves deeply interested in the concept. Moreover, we were already drawn to the idea of text-to-image conversion and its potential applications. We felt compelled to explore the intricacies of Giga GAN further and decided to undertake a project to reproduce the code and learn more about its text-to-image processing techniques.</p><p><figure id=figure-gigagan><div class="d-flex justify-content-center"><div class=w-100><img src=data/GigaGAN%20model.png alt=GigaGAN loading=lazy data-zoomable></div></div><figcaption>GigaGAN</figcaption></figure></p><h3 id=challenges-in-gigagan>Challenges in GigaGAN</h3><p>However, after reading the entire paper, we realized the serious shortage of computing resources. (The smallest version of Giga GAN in the paper is trained on 64 A-100 GPU.) So, we plan to only create a &ldquo;toy&rdquo; version of Giga GAN to showcase its implementation ideas. After referring to the GitHub code to implement the forward part of the model, we decided to select some parts from the full model in the original paper for experimentation. In the end, we chose clip loss, matching aware loss, multi-scale loss, and filter bank.</p><p><figure id=figure-gigagan-model-size><div class="d-flex justify-content-center"><div class=w-100><img src=./data/gigagan%20model%20size.png alt="GigaGAN model Size" loading=lazy data-zoomable></div></div><figcaption>GigaGAN Model Size</figcaption></figure></p><p>We have written the code for these parts based on the paper. However, even if we use a very small batch size (which is 2), the model could only be trained on our GPU when incorporating the matching aware loss and clip loss. When multi-scale loss or filter bank is added, the GPU&rsquo;s memory is insufficient.</p><p>The dataset we use is a Pokemon dataset with text and images paired. This text will provide a simple description of the image, with key information including color, species, and action.</p><p><figure id=figure-pokemon-dataset><div class="d-flex justify-content-center"><div class=w-100><img src=./data/pokemon_dataset_small.png alt="Pokemon Dataset" loading=lazy data-zoomable></div></div><figcaption>Pokemon Dataset</figcaption></figure></p><h3 id=results-in-gigagan>Results in GigaGAN</h3><p>In the beginning, we only added the most basic GAN loss without any condition to test the ability to generate clear images: that is, the model only pays attention to the authenticity of the generated images.</p><p><figure id=figure-pokemon-unconditional><div class="d-flex justify-content-center"><div class=w-100><img src=./data/pokemon%20unconditional.png alt="Pokemon Unconditional" loading=lazy data-zoomable></div></div><figcaption>Pokemon Unconditional</figcaption></figure></p><p>It can be seen that the generated image still has a rough outline, although the details are not satisfactory. Also, it can be seen that the colors, contours, and patterns all have certain rules, and the generated images have diversity.</p><p>Then we tried to add a na√Øve text condition to observe the results:</p><p><figure id=figure-drawing-of-a-brown-and-black-pokemon><div class="d-flex justify-content-center"><div class=w-100><img src=./data/drawing%20of%20a%20brown%20and%20black%20Pokemon.png alt="drawing of a brown and black Pokemon" loading=lazy data-zoomable></div></div><figcaption>drawing of a brown and black Pokemon</figcaption></figure><strong>drawing of a brown and black Pokemon</strong></p><p>It can be seen that the image quality has significantly decreased compared to not incorporating text conditions. It can be seen that the generated image roughly matches the content described in the text, but its shape is relatively strange. It means that the addition of text conditions significantly makes training more difficult to converge.</p><p>Then we added matching loss for training and observed the results. It can be seen that the colors described in the generated image and text can match, but the shape is relatively irregular, indicating that the training did not converge well.</p><p><figure id=figure-drawing-of-a-brown-and-black-pokemon><div class="d-flex justify-content-center"><div class=w-100><img src=./data/drawing%20of%20a%20brown%20and%20black%20Pokemon%202.png alt="drawing of a brown and black Pokemon 2" loading=lazy data-zoomable></div></div><figcaption>drawing of a brown and black Pokemon</figcaption></figure><strong>drawing of a brown and black Pokemon with matching loss</strong></p><p>Then we added clip loss for training and observed the results. The generated images have no meaning, this is a completely non-convergent model. This indicates that our designed clip loss makes training extremely difficult to converge.</p><p><figure id=figure-drawing-of-a-brown-and-black-pokemon-with-clip><div class="d-flex justify-content-center"><div class=w-100><img src=data/drawing%20of%20a%20brown%20and%20black%20Pokemon%20clip.png alt="drawing of a brown and black Pokemon 3" loading=lazy data-zoomable></div></div><figcaption>drawing of a brown and black Pokemon with clip</figcaption></figure><strong>drawing of a brown and black Pokemon with clip loss</strong></p><p>Besides the visual differences, the quantitative results also show that the clip loss did not work in our case, and even produce worse results.</p><p><figure id=figure-lpips><div class="d-flex justify-content-center"><div class=w-100><img src=./data/LPIPS.png alt="LPIPS Results" loading=lazy data-zoomable></div></div><figcaption>LPIPS</figcaption></figure></p><p>This experiment is done using <a href=https://github.com/richzhang/PerceptualSimilarity target=_blank rel=noopener><strong>Learned Perceptual Image Patch Similarity (LPIPS)</strong></a> for evaluate the simiarity in feature space.</p><h3 id=conclusions-in-gigagan>Conclusions in GigaGAN:</h3><ol><li>The effect of the image we generated is not good, which may be related to the selection of a hyperparameter and the size of the data. Firstly, choosing a smaller batch size can result in poor convergence of the entire training. Secondly, there are too many hyperparameters in this model, and we can only use a few tuning parameters due to our computing resources.</li><li>GANs, in general, require a substantial amount of computing resources for training and might be unstable when training on small datasets or with insufficient computational resources.</li></ol><h3 id=continue-explore-text-2-image>Continue Explore Text 2 Image</h3><p>Although the implementation effect of Giga GAN is not satisfactory, we are still quite interested in handling clip loss. Thus, we searched for relevant papers and open-source resources and decided to convert text to images again on the diffusion model, and compared its effectiveness with GAN.</p><h2 id=switch-to-imagen>Switch to Imagen</h2><p>In this project, we first attempted to reproduce and explore the GigaGAN model, uncovering its remarkable text-to-image processing techniques. Subsequently, we shifted our focus to the <strong>MinImagen</strong><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> architecture, an efficient text-to-image generation model capable of producing high-quality images. Our goal is to experiment with these two models, comparing their strengths and weaknesses, and ultimately share our findings and experiences in the world of image generation.</p><p><figure id=figure-imagen-model><div class="d-flex justify-content-center"><div class=w-100><img src=./data/Imagen_model_structure.png alt="Imagen Architechture" loading=lazy data-zoomable></div></div><figcaption>Imagen Model</figcaption></figure></p><!-- raw HTML omitted --><h3 id=implementation>Implementation</h3><p>We began by implementing the <strong>MinImagen</strong> model using the resources provided by <strong>AssemblyAI</strong> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. The key component of this implementation is the <strong>Diffusion</strong> part, which has the following characteristics compared with the <strong>GigaGAN</strong> in our training environment.:</p><ul><li>Training is fast ‚ö°</li><li>Sample image generation is slow üê¢</li><li>Convergence is fast üèÉ</li><li>Generated results are more meaningful üéØ</li></ul><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><h3 id=experimentation>Experimentation</h3><p>During the experimentation phase, we encountered some challenges:</p><ol><li><strong>Model Size</strong>: The original model with super resolution was too large to fit into a 12GB GPU for training, even with a batch size of 1.</li><li><strong>Logging</strong>: We used <strong>Weights & Biases (wandb)</strong> for logging our training process.</li></ol><p><figure id=figure-validation-loss><div class="d-flex justify-content-center"><div class=w-100><img alt="Imagen Training" srcset="/post/project/data/Imagen_training_hua41c1c38400bea9d0b4e8558892b5dc1_80236_6fc326ee699feb087a5a94cc4d401ee5.webp 400w,
/post/project/data/Imagen_training_hua41c1c38400bea9d0b4e8558892b5dc1_80236_30cc487228e658bd5aade5286e9048bc.webp 760w,
/post/project/data/Imagen_training_hua41c1c38400bea9d0b4e8558892b5dc1_80236_1200x1200_fit_q100_h2_lanczos_3.webp 1200w" src=/post/project/data/Imagen_training_hua41c1c38400bea9d0b4e8558892b5dc1_80236_6fc326ee699feb087a5a94cc4d401ee5.webp width=760 height=469 loading=lazy data-zoomable></div></div><figcaption>Validation Loss</figcaption></figure></p><ol start=3><li><strong>Remove Super Resolution</strong>: Due to the model&rsquo;s large size, we had to remove the super resolution layer for training.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#75715e>// Training parameters
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>{
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;text_embed_dim&#34;</span>: <span style=color:#66d9ef>null</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;channels&#34;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;timesteps&#34;</span>: <span style=color:#ae81ff>1000</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;cond_drop_prob&#34;</span>: <span style=color:#ae81ff>0.15</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;loss_type&#34;</span>: <span style=color:#e6db74>&#34;l2&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;lowres_sample_noise_level&#34;</span>: <span style=color:#ae81ff>0.2</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;auto_normalize_img&#34;</span>: <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;dynamic_thresholding_percentile&#34;</span>: <span style=color:#ae81ff>0.9</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;only_train_unet_number&#34;</span>: <span style=color:#66d9ef>null</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;image_sizes&#34;</span>: [
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>64</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;text_encoder_name&#34;</span>: <span style=color:#e6db74>&#34;t5_small&#34;</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:#75715e>// Model Size
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>{
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;dim&#34;</span>: <span style=color:#ae81ff>128</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;dim_mults&#34;</span>: [
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;channels&#34;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;channels_out&#34;</span>: <span style=color:#66d9ef>null</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;cond_dim&#34;</span>: <span style=color:#66d9ef>null</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;text_embed_dim&#34;</span>: <span style=color:#ae81ff>512</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;num_resnet_blocks&#34;</span>: <span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;layer_attns&#34;</span>: [
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;layer_cross_attns&#34;</span>: [
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>true</span>,
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>true</span>
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;attn_heads&#34;</span>: <span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;lowres_cond&#34;</span>: <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;memory_efficient&#34;</span>: <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;attend_at_middle&#34;</span>: <span style=color:#66d9ef>false</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Despite these challenges, our model was able to converge quickly, taking only 300 epochs with a batch size of 2 and a time step of 1000 to generate meaningful images. üåü</p><table><thead><tr><th style=text-align:center>Training Step</th><th style=text-align:center>Image</th></tr></thead><tbody><tr><td style=text-align:center>a blue and red pokemon 500</td><td style=text-align:center><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./data/imagen_500.png alt="Step 500 Image" loading=lazy data-zoomable></div></div></figure></td></tr><tr><td style=text-align:center>a blue and red pokemon 600</td><td style=text-align:center><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./data/imagen_600.png alt="Step 600 Image" loading=lazy data-zoomable></div></div></figure></td></tr><tr><td style=text-align:center>a blue and red pokemon 700</td><td style=text-align:center><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./data/imagen_700.png alt="Step 700 Image" loading=lazy data-zoomable></div></div></figure></td></tr><tr><td style=text-align:center>a blue and red pokemon 800</td><td style=text-align:center><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./data/imagen_800.png alt="Step 800 Image" loading=lazy data-zoomable></div></div></figure></td></tr></tbody></table><p>We think the quality of Imagen generated image is better than our previous results from GigaGAN, even in a very early training stage. One could think these images look more complete than the previous results, thus some extent more like a Pokemon.</p><table><thead><tr><th style=text-align:center>Text Prompt (text scale)</th><th style=text-align:center>Image</th></tr></thead><tbody><tr><td style=text-align:center>a drawing of a green pokemon with red eyes (0.1)</td><td style=text-align:center><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./data/text_scale_0.1.png alt="text scale = 0.1" loading=lazy data-zoomable></div></div></figure></td></tr><tr><td style=text-align:center>a drawing of a green pokemon with red eyes (3.0)</td><td style=text-align:center><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./data/text_scale_3.0.png alt="text scale = 3.0" loading=lazy data-zoomable></div></div></figure></td></tr><tr><td style=text-align:center>a drawing of a green pokemon with red eyes (5.0)</td><td style=text-align:center><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./data/text_scale_5.0.png alt="text scale = 5.0" loading=lazy data-zoomable></div></div></figure></td></tr></tbody></table><p>We could notice that with add more strength to the text scale, it starts to align with the text in some extent. Like when it is 0.1, it is just green. When add to 3.0, it shows a yellow color, which might be some blending information from &ldquo;blue&rdquo; and &ldquo;red&rdquo;. When add to 5.0, it shows a blend with yellow and green, thus we assume it catchs more information from the content step by step.</p><p>When text scale = 5.0, we could notice that the image quality is not good and has a lot of noise on the white background. This is due to the text scale is too high that we force the network to focus more on the text information, but not the quality side.</p><!-- raw HTML omitted --><h3 id=future-work->Future Work üí°</h3><p>We plan to add a <strong>super resolution layer</strong> in the future to further improve our image generation capabilities.</p><h3 id=conclusion>Conclusion</h3><p>Our project on implementing and experimenting with the MinImagen architecture for text-to-image generation has been successful. We were able to generate meaningful images from textual descriptions, overcoming challenges related to model size and training resources. We hope that our experience and findings can help others working on similar projects. üòÉ</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>GigaGAN (<a href=https://mingukkang.github.io/GigaGAN/ target=_blank rel=noopener>https://mingukkang.github.io/GigaGAN/</a>)&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>MinImagen: Build Your Own Imagen Text-to-Image Model (<a href=https://www.assemblyai.com/blog/minimagen-build-your-own-imagen-text-to-image-model/ target=_blank rel=noopener>https://www.assemblyai.com/blog/minimagen-build-your-own-imagen-text-to-image-model/</a>)&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>AssemblyAI-Examples/MinImagen GitHub Repository (<a href="https://github.com/AssemblyAI-Examples/MinImagen?ref=assemblyai.com" target=_blank rel=noopener>https://github.com/AssemblyAI-Examples/MinImagen?ref=assemblyai.com</a>)&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=article-tags><a class="badge badge-light" href=/tag/computer-vision/>Computer Vision</a>
<a class="badge badge-light" href=/tag/image-generation/>Image Generation</a>
<a class="badge badge-light" href=/tag/deep-learning/>Deep Learning</a>
<a class="badge badge-light" href=/tag/diffusion/>Diffusion</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Flinjiw.github.io%2Fpost%2Fproject%2F&text=Explore+Test+2+Image+Generating" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Flinjiw.github.io%2Fpost%2Fproject%2F&t=Explore+Test+2+Image+Generating" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Explore%20Test%202%20Image%20Generating&body=https%3A%2F%2Flinjiw.github.io%2Fpost%2Fproject%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Flinjiw.github.io%2Fpost%2Fproject%2F&title=Explore+Test+2+Image+Generating" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Explore+Test+2+Image+Generating%20https%3A%2F%2Flinjiw.github.io%2Fpost%2Fproject%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Flinjiw.github.io%2Fpost%2Fproject%2F&title=Explore+Test+2+Image+Generating" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://linjiw.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_huc61a9cf250f63b4f157dbff5a6cdbb31_21867_270x270_fill_q100_lanczos_center.jpg alt="Linji (Joey) Wang"></a><div class=media-body><h5 class=card-title><a href=https://linjiw.github.io/>Linji (Joey) Wang</a></h5><h6 class=card-subtitle>PhD Student in AI & Robotics</h6><ul class=network-icon aria-hidden=true><li><a href=mailto:joewwang@outlook.com><i class="fas fa-envelope"></i></a></li><li><a href=https://www.linkedin.com/in/linjiw/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://github.com/linjiw target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=/files/Linji%20Wang%20CV%20site-linji.pdf><i class="ai ai-cv"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">¬© 2025 Me. This work is licensed under <!-- raw HTML omitted -->CC BY NC ND 4.0<!-- raw HTML omitted --></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <!-- raw HTML omitted -->Wowchemy<!-- raw HTML omitted --> ‚Äî the free, <!-- raw HTML omitted -->open source<!-- raw HTML omitted --> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js></script></body></html>