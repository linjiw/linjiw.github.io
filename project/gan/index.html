<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: August 15, 2025 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.0f229d4b7ebad1917a9a357cba2effab.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Linji (Joey) Wang"><meta name=description content="In this assignment, we implemented two types of GANs - a Deep Convolutional GAN (DCGAN) and a CycleGAN. The DCGAN was trained to generate grumpy cats from random noise, while the CycleGAN was trained to convert between two types of cats (Grumpy and Russian Blue) and between apples and oranges. Both GANs were implemented with data augmentation and differentiable augmentation techniques."><link rel=alternate hreflang=en-us href=https://linjiw.github.io/project/gan/><link rel=canonical href=https://linjiw.github.io/project/gan/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hub02f11719440bf94480d1ab9e24c040b_151872_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hub02f11719440bf94480d1ab9e24c040b_151872_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:image" content="https://linjiw.github.io/project/gan/featured.png"><meta property="og:site_name" content="Linji Wang"><meta property="og:url" content="https://linjiw.github.io/project/gan/"><meta property="og:title" content="When Cats meet GANs | Linji Wang"><meta property="og:description" content="In this assignment, we implemented two types of GANs - a Deep Convolutional GAN (DCGAN) and a CycleGAN. The DCGAN was trained to generate grumpy cats from random noise, while the CycleGAN was trained to convert between two types of cats (Grumpy and Russian Blue) and between apples and oranges. Both GANs were implemented with data augmentation and differentiable augmentation techniques."><meta property="og:image" content="https://linjiw.github.io/project/gan/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2019-02-05T00:00:00+00:00"><meta property="article:modified_time" content="2019-09-05T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://linjiw.github.io/project/gan/"},"headline":"When Cats meet GANs","image":["https://linjiw.github.io/project/gan/featured.png"],"datePublished":"2019-02-05T00:00:00Z","dateModified":"2019-09-05T00:00:00Z","author":{"@type":"Person","name":"Linji (Joey) Wang"},"publisher":{"@type":"Organization","name":"Linji Wang","logo":{"@type":"ImageObject","url":"https://linjiw.github.io/media/icon_hub02f11719440bf94480d1ab9e24c040b_151872_192x192_fill_lanczos_center_3.png"}},"description":"In this assignment, we implemented two types of GANs - a Deep Convolutional GAN (DCGAN) and a CycleGAN. The DCGAN was trained to generate grumpy cats from random noise, while the CycleGAN was trained to convert between two types of cats (Grumpy and Russian Blue) and between apples and oranges. Both GANs were implemented with data augmentation and differentiable augmentation techniques."}</script><title>When Cats meet GANs | Linji Wang</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=8a5c126575c0999f87cfbb8e190c9af6><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Linji Wang</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Linji Wang</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/ai/><span>AI</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/robotics/><span>Robotics</span></a></li><li class=nav-item><a class=nav-link href=/sde/><span>Software</span></a></li><li class=nav-item><a class=nav-link href=/rl/><span>RL</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class="article article-project"><div class="article-container pt-3"><h1>When Cats meet GANs</h1><p class=page-subtitle>A Comprehensive Study on DCGANs and CycleGANs with Advanced Augmentation Techniques</p><div class=article-metadata><div><span class=author-highlighted>Linji (Joey) Wang</span></div><span class=article-date>Last updated on
Sep 5, 2019</span>
<span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/project/>Project</a></span></div></div><div class="article-header container featured-image-wrapper mt-4 mb-4" style=max-width:512px;max-height:256px><div style=position:relative><img src=/project/gan/featured_hu4288bdcdb3699d80052a4f5c670366f0_287815_1200x2500_fit_q100_h2_lanczos_3.webp width=512 height=256 alt class=featured-image>
<span class=article-header-caption>DCGAN Results</span></div></div><div class=article-container><div class=article-style><h2 id=introduction>Introduction</h2><p>In this assignment, we get hands-on experience coding and training GANs. This assignment includes two parts:</p><p>Implementing a Deep Convolutional GAN (DCGAN) to generate grumpy cats from samples of random noise.
Implementing a more complex GAN architecture called CycleGAN for the task of image-to-image translation. We train the CycleGAN to convert between different types of two kinds of cats (Grumpy and Russian Blue) and between apples and oranges.</p><h2 id=part-1-deep-convolutional-gan>Part 1: Deep Convolutional GAN</h2><p>For the first part of this assignment, we implement a slightly modified version of Deep Convolutional GAN (DCGAN).</p><h3 id=implement-data-augmentation>Implement Data Augmentation</h3><p>Implemented the deluxe version of data augmentation in &lsquo;data_loader.py&rsquo;.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>elif</span> opts<span style=color:#f92672>.</span>data_preprocess <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;deluxe&#39;</span>:
</span></span><span style=display:flex><span>        load_size <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>1.1</span> <span style=color:#f92672>*</span> opts<span style=color:#f92672>.</span>image_size)
</span></span><span style=display:flex><span>        osize <span style=color:#f92672>=</span> [load_size, load_size]
</span></span><span style=display:flex><span>        deluxe_transform <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>Compose([
</span></span><span style=display:flex><span>        transforms<span style=color:#f92672>.</span>Resize(opts<span style=color:#f92672>.</span>image_size, Image<span style=color:#f92672>.</span>BICUBIC),
</span></span><span style=display:flex><span>        transforms<span style=color:#f92672>.</span>RandomCrop(opts<span style=color:#f92672>.</span>image_size),
</span></span><span style=display:flex><span>        transforms<span style=color:#f92672>.</span>RandomHorizontalFlip(),
</span></span><span style=display:flex><span>        transforms<span style=color:#f92672>.</span>ToTensor(),
</span></span><span style=display:flex><span>        transforms<span style=color:#f92672>.</span>Normalize((<span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.5</span>), (<span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.5</span>, <span style=color:#ae81ff>0.5</span>)),
</span></span><span style=display:flex><span>        ])
</span></span><span style=display:flex><span>        train_transform <span style=color:#f92672>=</span> deluxe_transform
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>pass</span>
</span></span></code></pre></div><h3 id=implement-the-discriminator-of-the-dcgan>Implement the Discriminator of the DCGAN</h3><p>(Answer for padding calculation goes here)</p><p>Implemented the architecture by filling in the &lsquo;<strong>init</strong>&rsquo; and &lsquo;forward&rsquo; method of the &lsquo;DCDiscriminator&rsquo; class in &lsquo;models.py&rsquo;.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> __init__(self, conv_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, norm<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;instance&#39;</span>):
</span></span><span style=display:flex><span>    super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>conv1 <span style=color:#f92672>=</span> conv(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, norm, <span style=color:#66d9ef>False</span>, <span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>conv2 <span style=color:#f92672>=</span> conv(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, norm, <span style=color:#66d9ef>False</span>, <span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>conv3 <span style=color:#f92672>=</span> conv(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, norm, <span style=color:#66d9ef>False</span>, <span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>conv4 <span style=color:#f92672>=</span> conv(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, norm, <span style=color:#66d9ef>False</span>, <span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>conv5 <span style=color:#f92672>=</span> conv(<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>0</span>, <span style=color:#66d9ef>None</span>, <span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Forward pass, x is (B, C, H, W).&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv1(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv2(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv3(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv4(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv5(x)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x<span style=color:#f92672>.</span>squeeze()
</span></span></code></pre></div><h3 id=generator>Generator</h3><p>Implemented the generator of the DCGAN by filling in the &lsquo;<strong>init</strong>&rsquo; and &lsquo;forward&rsquo; method of the &lsquo;DCGenerator&rsquo; class in &lsquo;models.py&rsquo;.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> __init__(self, noise_size, conv_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>):
</span></span><span style=display:flex><span>    super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>up_conv1 <span style=color:#f92672>=</span> conv(<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#e6db74>&#39;instance&#39;</span>, <span style=color:#66d9ef>False</span>,<span style=color:#e6db74>&#39;relu&#39;</span> )
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>up_conv2 <span style=color:#f92672>=</span> up_conv(<span style=color:#ae81ff>256</span>, <span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, scale_factor<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, norm<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;instance&#39;</span>, activ<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>up_conv3 <span style=color:#f92672>=</span> up_conv(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, scale_factor<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, norm<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;instance&#39;</span>, activ<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>up_conv4 <span style=color:#f92672>=</span> up_conv(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, scale_factor<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, norm<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;instance&#39;</span>, activ<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>up_conv5 <span style=color:#f92672>=</span> up_conv(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, scale_factor<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, norm<span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>, activ<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tanh&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, z):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Generate an image given a sample of random noise.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Input
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    -----
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        z: BS x noise_size x 1 x 1   --&gt;  16x100x1x1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Output
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        out: BS x channels x image_width x image_height  --&gt;  16x3x64x64
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>up_conv1(z)
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>up_conv2(z)
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>up_conv3(z)
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>up_conv4(z)
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>up_conv5(z)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> z
</span></span></code></pre></div><h3 id=training-loop>Training Loop</h3><p>Implemented the training loop for the DCGAN by filling in the indicated parts of the training_loop function in vanilla_gan.py.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>            <span style=color:#75715e># TRAIN THE DISCRIMINATOR</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 1. Compute the discriminator loss on real images</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> opts<span style=color:#f92672>.</span>use_diffaug:
</span></span><span style=display:flex><span>                D_real_loss <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean((D(DiffAugment(real_images, policy<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;color,translation,cutout&#39;</span>, channels_first<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span> )) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                D_real_loss <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean((D(real_images) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 2. Sample noise</span>
</span></span><span style=display:flex><span>            noise <span style=color:#f92672>=</span> sample_noise(opts<span style=color:#f92672>.</span>batch_size, opts<span style=color:#f92672>.</span>noise_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 3. Generate fake images from the noise</span>
</span></span><span style=display:flex><span>            fake_images <span style=color:#f92672>=</span> G(noise)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 4. Compute the discriminator loss on the fake images</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> opts<span style=color:#f92672>.</span>use_diffaug:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                D_fake_loss <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean((D(DiffAugment(fake_images<span style=color:#f92672>.</span>detach(), policy<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;color,translation,cutout&#39;</span>, channels_first<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span> ))) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                D_real_loss <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean((D(fake_images<span style=color:#f92672>.</span>detach())) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>            D_total_loss <span style=color:#f92672>=</span> (D_real_loss <span style=color:#f92672>+</span> D_fake_loss) <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># update the discriminator D</span>
</span></span><span style=display:flex><span>            d_optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>            D_total_loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>            d_optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># TRAIN THE GENERATOR</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 1. Sample noise</span>
</span></span><span style=display:flex><span>            noise <span style=color:#f92672>=</span> sample_noise(opts<span style=color:#f92672>.</span>batch_size, opts<span style=color:#f92672>.</span>noise_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 2. Generate fake images from the noise</span>
</span></span><span style=display:flex><span>            fake_images <span style=color:#f92672>=</span> G(noise)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># 3. Compute the generator loss</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> opts<span style=color:#f92672>.</span>use_diffaug:
</span></span><span style=display:flex><span>                G_loss <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean((D(DiffAugment(fake_images, policy<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;color,translation,cutout&#39;</span>, channels_first<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span> ))<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>                G_loss <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mean((D(fake_images)<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span></code></pre></div><h4 id=differentiable-augmentation>Differentiable Augmentation</h4><p>(Discussion of results with and without applying differentiable augmentations, and the difference between two augmentation schemes in terms of implementation and effects)</p><h3 id=experiment-with-dcgans>Experiment with DCGANs</h3><p>INSERT IMAGE: Screenshots of discriminator and generator training loss with &ndash;data_preprocess=basic, &ndash;data_preprocess=deluxe.</p><p>(Brief explanation of what the curves should look like if GAN manages to train)</p><p>INSERT IMAGE: With &ndash;data_preprocess=deluxe and differentiable augmentation enabled, show one of the samples from early in training (e.g., iteration 200) and one of the samples from later in training, and give the iteration number for those samples.</p><p>(Brief comment on the quality of the samples, and in what way they improve through training)</p><h2 id=part-2-cyclegan>Part 2: CycleGAN</h2><p>Implemented the CycleGAN architecture.</p><h3 id=data-augmentation>Data Augmentation</h3><p>Set the &ndash;data_preprocess flag to deluxe.</p><h3 id=generator-1>Generator</h3><p>Implemented the generator architecture by completing the <strong>init</strong> method of the CycleGenerator class in models.py.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> __init__(self, conv_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>, init_zero_weights<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, norm<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;instance&#39;</span>):
</span></span><span style=display:flex><span>    super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># # 1. Define the encoder part of the generator</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>conv1 <span style=color:#f92672>=</span> conv(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, norm, <span style=color:#66d9ef>False</span>, <span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>conv2 <span style=color:#f92672>=</span> conv(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>, norm, <span style=color:#66d9ef>False</span>, <span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># # 2. Define the transformation part of the generator</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>resnet_block <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(ResnetBlock(conv_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>, norm <span style=color:#f92672>=</span> norm, activ <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>                                      ResnetBlock(conv_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>, norm <span style=color:#f92672>=</span> norm, activ <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>                                      ResnetBlock(conv_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>64</span>, norm <span style=color:#f92672>=</span> norm, activ <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;relu&#39;</span>),)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># # 3. Define the decoder part of the generator</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>up_conv1 <span style=color:#f92672>=</span> up_conv(<span style=color:#ae81ff>64</span>, <span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, scale_factor<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, norm<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;instance&#39;</span>, activ<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>up_conv2 <span style=color:#f92672>=</span> up_conv(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>, stride<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, scale_factor<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, norm<span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>, activ<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tanh&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Generate an image conditioned on an input image.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Input
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    -----
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        x: BS x 3 x 32 x 32
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Output
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        out: BS x 3 x 32 x 32
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv1(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>conv2(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>resnet_block(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>up_conv1(x)
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>up_conv2(x)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><h3 id=training-loop-1>Training Loop</h3><p>Implemented the training loop for the CycleGAN by filling in the indicated parts of the training_loop function in cycle_gan.py.</p><h3 id=experiment-with-cyclegan>Experiment with CycleGAN</h3><p>INSERT IMAGE: Two example images of generated Grumpy cats from Russian Blue cats, and two example images of generated Russian Blue cats from Grumpy cats.</p><p>(Brief comment on the quality of the generated images, and whether the CycleGAN has captured the main differences between the two domains)</p><p>INSERT IMAGE: Two example images of generated apples from oranges, and two example images of generated oranges from apples.</p><p>(Brief comment on the quality of the generated images, and whether the CycleGAN has captured the main differences between the two domains)</p><h2 id=conclusion>Conclusion</h2><p>This report presents our implementation of DCGAN and CycleGAN for various image generation tasks. Through these experiments, we have observed the impact of data augmentation and differentiable augmentation on the training process and final results. We have also seen the capabilities of CycleGAN in generating realistic images for domain-to-domain translation tasks, such as converting Grumpy cats to Russian Blue cats and vice versa, and converting apples to oranges and vice versa.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/computer-vision/>Computer Vision</a>
<a class="badge badge-light" href=/tag/image-generation/>Image Generation</a>
<a class="badge badge-light" href=/tag/deep-learning/>Deep Learning</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Flinjiw.github.io%2Fproject%2Fgan%2F&text=When+Cats+meet+GANs" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Flinjiw.github.io%2Fproject%2Fgan%2F&t=When+Cats+meet+GANs" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=When%20Cats%20meet%20GANs&body=https%3A%2F%2Flinjiw.github.io%2Fproject%2Fgan%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Flinjiw.github.io%2Fproject%2Fgan%2F&title=When+Cats+meet+GANs" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=When+Cats+meet+GANs%20https%3A%2F%2Flinjiw.github.io%2Fproject%2Fgan%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Flinjiw.github.io%2Fproject%2Fgan%2F&title=When+Cats+meet+GANs" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://linjiw.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_huc61a9cf250f63b4f157dbff5a6cdbb31_21867_270x270_fill_q100_lanczos_center.jpg alt="Linji (Joey) Wang"></a><div class=media-body><h5 class=card-title><a href=https://linjiw.github.io/>Linji (Joey) Wang</a></h5><h6 class=card-subtitle>PhD Student in AI & Robotics</h6><ul class=network-icon aria-hidden=true><li><a href=mailto:joewwang@outlook.com><i class="fas fa-envelope"></i></a></li><li><a href=https://www.linkedin.com/in/linjiw/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=https://github.com/linjiw target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=/files/Linji%20Wang%20CV%20site-linji.pdf><i class="ai ai-cv"></i></a></li></ul></div></div><div class="project-related-pages content-widget-hr"></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 Me. This work is licensed under <!-- raw HTML omitted -->CC BY NC ND 4.0<!-- raw HTML omitted --></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <!-- raw HTML omitted -->Wowchemy<!-- raw HTML omitted --> — the free, <!-- raw HTML omitted -->open source<!-- raw HTML omitted --> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js></script></body></html>